# /root/ICML2026/event_graph/run_inference.py
import argparse
import os
import json
from my_datasets import DATASET_REGISTRY
from code import METHOD_REGISTRY

# å¼•å…¥æˆ‘ä»¬åˆšæ‰å†™çš„ VideoLLaVAWrapper
from models.video_llava_7b import VideoLLaVAWrapper

def load_model(backbone_name):
    """
    æ¨¡å‹åŠ è½½å·¥å‚å‡½æ•°
    """
    if backbone_name == "Video-LLaVA-7B":
        from models.video_llava_7b import VideoLLaVAWrapper
        return VideoLLaVAWrapper()
    elif backbone_name == "LLaVA-NeXT-Video-34B":
        # â­ æ–°å¢ï¼š34Bæ¨¡å‹æ”¯æŒ
        from models.llava_next_34b import LLaVANext34BWrapper
        return LLaVANext34BWrapper()
    else:
        raise ValueError(f"Unknown backbone: {backbone_name}")

def parse_args():
    parser = argparse.ArgumentParser(description="EventGraph-LLM Experiments")
    
    # æ ¸å¿ƒé€‰æ‹©å‚æ•°
    parser.add_argument("--dataset", type=str, required=True, choices=["VideoMME", "LongVideoBench", "MLVU", "CinePile"])
    parser.add_argument("--method", type=str, required=True, choices=["EventGraph-LMM"])
    parser.add_argument("--backbone", type=str, default="Video-LLaVA-7B", choices=["Video-LLaVA-7B", "LLaVA-NeXT-Video-34B"])
    
    # --- [æ–°å¢] å¹¶è¡Œåˆ†ç‰‡å‚æ•° (ç”¨äºå¤šå¡å¹¶è¡Œ) ---
    parser.add_argument("--num_chunks", type=int, default=1, help="æŠŠæ•°æ®é›†åˆ†æˆå‡ ä»½")
    parser.add_argument("--chunk_idx", type=int, default=0, help="å½“å‰è·‘ç¬¬å‡ ä»½ (0 åˆ° num_chunks-1)")
    # ---------------------------------------
    
    # è·¯å¾„ä¸è¶…å‚
    parser.add_argument("--data_root", type=str, default="/root/ICML2026/dataset")
    parser.add_argument("--token_budget", type=int, default=2048)
    
    
    # â­ æ–°å¢ï¼šé™åˆ¶æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    parser.add_argument("--max_samples", type=int, default=None,
                        help="Limit number of samples for quick testing (default: None for all)")
    
    parser.add_argument("--top_k_longest", type=int, default=None,
                        help="Select top K longest videos (overrides max_samples)")
    
    # === GNU Parallelæ‰¹å¤„ç†æ¨¡å¼å‚æ•° (å¯é€‰ï¼Œå‘åå…¼å®¹) ===
    parser.add_argument("--batch_mode", action="store_true",
                        help="Enable batch processing mode for GNU Parallel dynamic load balancing")
    parser.add_argument("--sample_indices", type=str, default=None,
                        help="Comma-separated sample indices for batch processing (e.g., '0,1,2,3,4')")
    # =====================================================

    # è¾“å‡ºç›®å½•
    parser.add_argument("--output_dir", type=str, default="./result")
    
    return parser.parse_args()

def main():
    args = parse_args()
    
    # 1. åŠ è½½æ¨¡å‹ (Backbone)
    # âš ï¸ ç‰¹æ®Šå¤„ç†ï¼šFastVã€DyCokeã€ToMe(å®Œæ•´ç‰ˆ)ä½¿ç”¨ç‹¬ç«‹çš„model wrapper
    if args.method in ["FastV", "DyCoke"]:
        print(f"ğŸš€ [1/4] Skipping main model load ({args.method} uses isolated model)...")
        model = None  # FastV/DyCokeä¼šåœ¨__init__ä¸­åŠ è½½è‡ªå·±çš„model
    elif args.method == "ToMe" and args.use_full_tome:
        # â­ ToMeå®Œæ•´ç‰ˆï¼šä½¿ç”¨VideoLLaVATomeWrapper
        print(f"ğŸš€ [1/4] Loading ToMe-patched Backbone: {args.backbone}...")
        from models.video_llava_7b_tome import VideoLLaVATomeWrapper
        model = VideoLLaVATomeWrapper(
            model_path="/root/hhq/models/Video-LLaVA-7B-hf",
            token_budget=args.token_budget,
            num_frames=32
        )
        print(f"   âœ… VideoLLaVATomeWrapper loaded with token budget={args.token_budget}")
    else:
        print(f"ğŸš€ [1/4] Loading Backbone: {args.backbone}...")
        model = load_model(args.backbone)
    
    # 2. åˆå§‹åŒ–æ–¹æ³• (Method)
    print(f"ğŸ› ï¸ [2/4] Initializing Method: {args.method}...")
    method_class = METHOD_REGISTRY[args.method]
    processor = method_class(args, model)
    
    # 3. åŠ è½½æ•°æ®é›†
    print(f"ğŸ“‚ [3/4] Loading Dataset: {args.dataset} (Mode: {args.duration_mode})...")
    dataset_class = DATASET_REGISTRY[args.dataset]
    
    # --- [ä¿®æ”¹] é’ˆå¯¹ VideoMME ä¼ å…¥ duration_mode ---
    if args.dataset == "VideoMME":
        # VideoMME ç±»æ”¯æŒ duration_mode å‚æ•°
        dataset = dataset_class(root_dir=args.data_root, duration_mode=args.duration_mode)
    else:
        # å…¶ä»–æ•°æ®é›†æš‚æœªå®ç°è¯¥ç­›é€‰åŠŸèƒ½ï¼ŒæŒ‰é»˜è®¤åŠ è½½
        dataset = dataset_class(root_dir=args.data_root)
    
    # === [æ–°å¢] Top K Longest é€»è¾‘ ===
    if args.top_k_longest is not None and hasattr(dataset, 'samples'):
        print(f"ğŸ“ [Filtering] Sorting by duration to get top {args.top_k_longest} longest videos...")
        # å‡è®¾ VideoMME çš„ metadata é‡ŒåŒ…å« 'duration' å­—æ®µ
        # è¿™é‡Œçš„ 0 æ˜¯é»˜è®¤å€¼ï¼Œé˜²æ­¢æŸäº›æ ·æœ¬æ²¡æœ‰ duration æŠ¥é”™
        dataset.samples.sort(key=lambda x: x.get('duration', 0), reverse=True)
        dataset.samples = dataset.samples[:args.top_k_longest]
        print(f"   âœ… Filtered down to {len(dataset.samples)} longest samples.")
    # ===================================
    
    # === [æ–°å¢] é™åˆ¶æ ·æœ¬æ•° (ç”¨äºå¿«é€Ÿæµ‹è¯•) ===
    # â­ æ³¨æ„ï¼šå¿…é¡»å…ˆé™åˆ¶æ€»æ ·æœ¬æ•°ï¼Œå†åˆ†ç‰‡ï¼
    if args.max_samples is not None:
        if hasattr(dataset, 'samples'):
            original_count = len(dataset.samples)
            dataset.samples = dataset.samples[:args.max_samples]
            print(f"ğŸ”¬ [Testing Mode] Limited to {args.max_samples} samples (Original: {original_count})")
        else:
            print("âš ï¸ Warning: Dataset does not support max_samples limiting.")
    # ==========================================
    
    # === [æ–°å¢] GNU Parallelæ‰¹å¤„ç†æ¨¡å¼ (ä¼˜å…ˆçº§é«˜äºchunkæ¨¡å¼) ===
    if args.batch_mode and args.sample_indices:
        # æ‰¹å¤„ç†æ¨¡å¼ï¼šå¤„ç†æŒ‡å®šçš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        indices = [int(i.strip()) for i in args.sample_indices.split(',') if i.strip()]
        if hasattr(dataset, 'samples'):
            total_samples = len(dataset.samples)
            # è¿‡æ»¤æ— æ•ˆç´¢å¼•
            valid_indices = [i for i in indices if 0 <= i < total_samples]
            if len(valid_indices) < len(indices):
                print(f"âš ï¸ Warning: {len(indices) - len(valid_indices)} invalid indices filtered")
            
            dataset.samples = [dataset.samples[i] for i in valid_indices]
            print(f"ğŸ”‹ [Batch Mode] Processing {len(valid_indices)} samples: {valid_indices[:5]}...")
        else:
            print("âš ï¸ Warning: Dataset does not support batch mode. Falling back to standard mode.")
    # === [åŸæœ‰] æ•°æ®åˆ†ç‰‡é€»è¾‘ (ç”¨äºå¤šå¡å¹¶è¡Œ) ===
    elif args.num_chunks > 1:
        total_samples = len(dataset)
        chunk_size = total_samples // args.num_chunks
        start_idx = args.chunk_idx * chunk_size
        
        # å¤„ç†æœ€åä¸€ä»½ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å‰©ä½™æ•°æ®
        if args.chunk_idx == args.num_chunks - 1:
            end_idx = total_samples
        else:
            end_idx = (args.chunk_idx + 1) * chunk_size
        
        # æ‰§è¡Œåˆ‡ç‰‡
        if hasattr(dataset, 'samples'):
            dataset.samples = dataset.samples[start_idx:end_idx]
            print(f"ğŸ”„ [Parallel] Running Chunk {args.chunk_idx}/{args.num_chunks}: Samples {start_idx} to {end_idx} (Total {len(dataset)})")
        else:
            print("âš ï¸ Warning: Dataset does not support list slicing. Sharding skipped.")
    # ==========================================

    # 4. å¾ªç¯æ¨ç†
    print(f"â–¶ï¸ [4/4] Start Inference...")
    print(f"  ğŸ“Š Total samples: {len(dataset)}")
    print(f"  ğŸ¯ Method: {args.method}")
    print(f"  ğŸ’¾ Results will be saved to: {args.output_dir}")
    print()
    
    results = []
    
    # ä½¿ç”¨tqdmæ·»åŠ è¿›åº¦æ¡
    from tqdm import tqdm
    
    for idx, sample in enumerate(tqdm(dataset, desc="Processing", unit="sample")):
        try:
            # sample åŒ…å«: {'video_path':..., 'question':..., 'options':...}
            pred_answer = processor.process_and_inference(
                sample['video_path'], 
                sample['question'], 
                sample.get('options', [])
            )
            
            results.append({
                "id": sample['id'],
                "pred": pred_answer,
                "gt": sample['answer']
            })
            
            # æ‰“å°ç»“æœï¼ˆåªæ‰“å°å‰10ä¸ªæ ·æœ¬ï¼Œé¿å…è¾“å‡ºè¿‡å¤šï¼‰
            if idx < 10:
                tqdm.write(f"  âœ“ Sample {sample['id']}: Pred={pred_answer} | GT={sample['answer']}")
            
        except Exception as e:
            tqdm.write(f"  âŒ Error in sample {sample['id']}: {e}")
            # æ·»åŠ åˆ°ç»“æœä¸­æ ‡è®°ä¸ºé”™è¯¯
            results.append({
                "id": sample['id'],
                "pred": "ERROR",
                "gt": sample['answer'],
                "error": str(e)
            })
            # import traceback
            # traceback.print_exc() # è°ƒè¯•æ—¶å¯ä»¥å–æ¶ˆæ³¨é‡ŠæŸ¥çœ‹è¯¦ç»†æŠ¥é”™
    
    print()        
    # ä¿å­˜ç»“æœ
    os.makedirs(args.output_dir, exist_ok=True)
    
    # æ–‡ä»¶ååŠ ä¸Š duration_mode å’Œ chunk_idx (å¦‚æœæœ‰åˆ†ç‰‡) æ–¹ä¾¿åŒºåˆ†
    suffix = ""
    if args.num_chunks > 1:
        suffix = f"_chunk{args.chunk_idx}"
        
    save_file = os.path.join(args.output_dir, f"{args.dataset}_{args.method}_{args.duration_mode}_{args.backbone}{suffix}.json")
    
    with open(save_file, 'w') as f:
        json.dump(results, f, indent=4)
    
    # ç»Ÿè®¡æ­£ç¡®ç‡
    correct = sum(1 for r in results if r.get('pred') == r.get('gt') and r.get('pred') != "ERROR")
    total = len([r for r in results if r.get('pred') != "ERROR"])
    accuracy = correct / total * 100 if total > 0 else 0
    
    print(f"âœ… Done! Results saved to {save_file}")
    print(f"ğŸ“ˆ Accuracy: {correct}/{total} = {accuracy:.2f}%")
    print(f"âŒ Errors: {len([r for r in results if r.get('pred') == 'ERROR'])}")

if __name__ == "__main__":
    main()