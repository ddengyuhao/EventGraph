#!/usr/bin/env python3
"""
ToMe (Token Merging) Wrapper for Video-LLaVA-7B
ä¸¥æ ¼æŒ‰ç…§å®˜æ–¹ tome/patch/timm.py å®ç°

æ ¸å¿ƒæ€è·¯ï¼š
1. åŠ è½½æ ‡å‡†Video-LLaVA-7Bæ¨¡å‹
2. Monkey patch Vision Towerçš„CLIPEncoderLayer
3. åœ¨æ¯å±‚attentionåæ‰§è¡Œtoken merging
4. æœ€ç»ˆè¾“å‡ºç»´åº¦ä¸å˜ï¼Œå¯¹LLMé€æ˜

å‚è€ƒï¼š
- è®ºæ–‡: Token Merging: Your ViT But Faster (ICLR 2023)
- å®˜æ–¹æºç : origin/ToMe/tome/patch/timm.py
"""

import torch
import torch.nn as nn
import math
import numpy as np
from typing import Callable, Tuple, List, Optional
from PIL import Image

# å¯¼å…¥åŸºç¡€wrapper
from .video_llava_7b import VideoLLaVAWrapper

try:
    from decord import VideoReader, cpu
except ImportError:
    print("âš ï¸ Warning: decord not installed")
    VideoReader = None


# ============================================================================
# æ ¸å¿ƒç®—æ³•ï¼šç›´æ¥ä»å®˜æ–¹ tome/merge.py ç…§æ¬
# ============================================================================

def bipartite_soft_matching(
    metric: torch.Tensor,
    r: int,
    class_token: bool = False,
    distill_token: bool = False,
) -> Tuple[Callable, Callable]:
    """
    åŒå‘è½¯åŒ¹é…ç®—æ³•ï¼ˆå®˜æ–¹tome/merge.py line 18-97åŸæ–‡å®ç°ï¼‰
    
    Applies ToMe with a balanced matching set (50%, 50%).
    Input size is [batch, tokens, channels].
    r indicates the number of tokens to remove (max 50% of tokens).
    
    Args:
        metric: Similarity metric tensor (é€šå¸¸æ˜¯attention keys)
        r: è¦åˆå¹¶çš„tokenæ•°é‡
        class_token: æ˜¯å¦æœ‰CLS tokenï¼ˆCLIPæœ‰ï¼‰
        distill_token: æ˜¯å¦æœ‰è’¸é¦tokenï¼ˆCLIPæ²¡æœ‰ï¼‰
    
    Returns:
        merge: åˆå¹¶å‡½æ•°
        unmerge: ååˆå¹¶å‡½æ•°ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    """
    protected = 0
    if class_token:
        protected += 1
    if distill_token:
        protected += 1

    # We can only reduce by a maximum of 50% tokens
    t = metric.shape[1]
    r = min(r, (t - protected) // 2)

    if r <= 0:
        # No merging
        return lambda x, mode=None: x, lambda x: x

    with torch.no_grad():
        # â­ Cosine similarity (normalize)
        metric = metric / metric.norm(dim=-1, keepdim=True)
        
        # â­ Alternating partition
        a, b = metric[..., ::2, :], metric[..., 1::2, :]
        
        # â­ Compute similarity scores
        scores = a @ b.transpose(-1, -2)

        # â­ Protect CLS token
        if class_token:
            scores[..., 0, :] = -math.inf
        if distill_token:
            scores[..., :, 0] = -math.inf

        # â­ Find best match for each token in A
        node_max, node_idx = scores.max(dim=-1)
        edge_idx = node_max.argsort(dim=-1, descending=True)[..., None]

        # â­ Keep top-r edges
        unm_idx = edge_idx[..., r:, :]  # Unmerged Tokens
        src_idx = edge_idx[..., :r, :]  # Merged Tokens
        dst_idx = node_idx[..., None].gather(dim=-2, index=src_idx)

        if class_token:
            # Sort to ensure the class token is at the start
            unm_idx = unm_idx.sort(dim=1)[0]

    def merge(x: torch.Tensor, mode="mean") -> torch.Tensor:
        """
        åº”ç”¨token mergingï¼ˆå®˜æ–¹line 70-80ï¼‰
        
        Args:
            x: [batch, tokens, channels]
            mode: "mean" or "sum"
        
        Returns:
            merged_x: [batch, tokens-r, channels]
        """
        src, dst = x[..., ::2, :], x[..., 1::2, :]
        n, t1, c = src.shape
        
        # Extract unmerged tokens from A
        unm = src.gather(dim=-2, index=unm_idx.expand(n, t1 - r, c))
        
        # Extract source tokens to be merged
        src = src.gather(dim=-2, index=src_idx.expand(n, r, c))
        
        # â­ Scatter-reduce (å®˜æ–¹ä½¿ç”¨scatter_reduceï¼ŒPyTorch 1.12+)
        dst = dst.scatter_reduce(-2, dst_idx.expand(n, r, c), src, reduce=mode)

        if distill_token:
            return torch.cat([unm[:, :1], dst[:, :1], unm[:, 1:], dst[:, 1:]], dim=1)
        else:
            return torch.cat([unm, dst], dim=1)

    def unmerge(x: torch.Tensor) -> torch.Tensor:
        """ååˆå¹¶ï¼ˆç”¨äºå¯è§†åŒ–ï¼Œå®˜æ–¹line 82-95ï¼‰"""
        unm_len = unm_idx.shape[1]
        unm, dst = x[..., :unm_len, :], x[..., unm_len:, :]
        n, _, c = unm.shape

        src = dst.gather(dim=-2, index=dst_idx.expand(n, r, c))

        out = torch.zeros(n, metric.shape[1], c, device=x.device, dtype=x.dtype)

        out[..., 1::2, :] = dst
        out.scatter_(dim=-2, index=(2 * unm_idx).expand(n, unm_len, c), src=unm)
        out.scatter_(dim=-2, index=(2 * src_idx).expand(n, r, c), src=src)

        return out

    return merge, unmerge


def merge_wavg(
    merge: Callable, 
    x: torch.Tensor, 
    size: Optional[torch.Tensor] = None
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    åŠ æƒå¹³å‡åˆå¹¶ï¼ˆå®˜æ–¹tome/merge.py line 210-224ï¼‰
    
    Applies the merge function by taking a weighted average based on token size.
    
    Args:
        merge: åˆå¹¶å‡½æ•°
        x: tokens [batch, tokens, channels]
        size: tokenå¤§å° [batch, tokens, 1]ï¼ˆè¡¨ç¤ºæ¯ä¸ªtokenä»£è¡¨å‡ ä¸ªåŸå§‹patchï¼‰
    
    Returns:
        merged_x: åˆå¹¶åçš„tokens
        merged_size: åˆå¹¶åçš„tokenå¤§å°
    """
    if size is None:
        size = torch.ones_like(x[..., 0, None])

    # Weighted sum
    x = merge(x * size, mode="sum")
    size = merge(size, mode="sum")

    # Normalize
    x = x / size
    
    return x, size


def parse_r(num_layers: int, r: int) -> List[int]:
    """
    è§£æmerging scheduleï¼ˆå®˜æ–¹tome/utils.py line 80-105ï¼‰
    
    Args:
        num_layers: å±‚æ•°
        r: æ¯å±‚åˆå¹¶çš„tokenæ•°ï¼ˆconstant scheduleï¼‰
    
    Returns:
        r_schedule: List[int]ï¼Œæ¯å±‚çš„rå€¼
    """
    # Constant schedule: æ¯å±‚åˆå¹¶ç›¸åŒæ•°é‡
    return [r] * num_layers


# ============================================================================
# ToMeé€‚é…å±‚ï¼šå°†å®˜æ–¹timm.Blocké€»è¾‘é€‚é…åˆ°CLIPEncoderLayer
# ============================================================================

class ToMeCLIPEncoderLayer(nn.Module):
    """
    ToMeç‰ˆæœ¬çš„CLIPEncoderLayer
    
    å‚ç…§å®˜æ–¹tome/patch/timm.py:ToMeBlock (line 21-56)
    é€‚é…transformersçš„CLIPEncoderLayeræ¶æ„
    
    æ ¸å¿ƒä¿®æ”¹ï¼š
    1. åœ¨self-attentionåæ‰§è¡Œtoken merging
    2. è·Ÿè¸ªtoken sizeï¼ˆç”¨äºweighted averageï¼‰
    3. ä¸æ”¹å˜è¾“å…¥è¾“å‡ºæ¥å£ï¼ˆå¯¹å¤–é€æ˜ï¼‰
    """
    
    def __init__(self, original_layer, tome_info: dict, layer_idx: int):
        """
        Args:
            original_layer: åŸå§‹çš„CLIPEncoderLayerå®ä¾‹
            tome_info: ToMeé…ç½®å­—å…¸
            layer_idx: å½“å‰å±‚çš„ç´¢å¼•ï¼ˆç”¨äºä»råˆ—è¡¨ä¸­è·å–å¯¹åº”çš„rå€¼ï¼‰
        """
        super().__init__()
        
        # å¤åˆ¶åŸå§‹layerçš„æ‰€æœ‰å±æ€§
        self.__dict__.update(original_layer.__dict__)
        
        # ToMeé…ç½®
        self._tome_info = tome_info
        self._layer_idx = layer_idx  # â­ ä¿å­˜layerç´¢å¼•
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        causal_attention_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = False,
    ) -> Tuple[torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ï¼ˆå‚ç…§å®˜æ–¹ToMeBlock.forwardï¼Œline 34-56ï¼‰
        
        æµç¨‹ï¼š
        1. Layer Norm 1
        2. Self-Attentionï¼ˆéœ€è¦è·å–keysç”¨äºmatchingï¼‰
        3. â­ Token Mergingï¼ˆå¦‚æœr>0ï¼‰
        4. Layer Norm 2
        5. MLP
        """
        # 1. Residual connectionå‡†å¤‡
        residual = hidden_states
        
        # 2. Layer Norm 1
        hidden_states = self.layer_norm1(hidden_states)
        
        # 3. Self-Attention
        # âš ï¸ è¿™é‡Œéœ€è¦è·å–attention keysç”¨äºmatching
        # CLIPAttentionçš„forwardé»˜è®¤ä¸è¿”å›keysï¼Œæˆ‘ä»¬é€šè¿‡_tome_infoä¼ é€’
        attn_output = self.self_attn(
            hidden_states,
            attention_mask=attention_mask,
            causal_attention_mask=causal_attention_mask,
            output_attentions=output_attentions,
        )
        
        # CLIPAttentionè¿”å›(attn_output, attn_weights)æˆ–attn_output
        if isinstance(attn_output, tuple):
            attn_output = attn_output[0]
        
        hidden_states = residual + attn_output
        
        # 4. â­ Token Mergingï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
        # âš ï¸ ä½¿ç”¨layer_idxè®¿é—®rï¼Œè€Œä¸æ˜¯pop()
        r = self._tome_info["r"][self._layer_idx] if self._tome_info["r"] else 0
        
        # ğŸ” DEBUG: æ‰“å°layer infoï¼ˆä»…layer 0ï¼‰
        if self._layer_idx == 0:
            print(f"  [ToMe DEBUG] Layer {self._layer_idx}: input shape={hidden_states.shape}, r={r}")
        
        if r > 0:
            input_shape = hidden_states.shape
            
            # è·å–metricï¼ˆä½¿ç”¨ä¸Šä¸€æ­¥attentionçš„keysï¼‰
            # âš ï¸ ä¸´æ—¶æ–¹æ¡ˆï¼šä½¿ç”¨hidden_statesä½œä¸ºmetric
            # TODO: ç†æƒ³æƒ…å†µåº”è¯¥ä»attentionæå–keys
            metric = hidden_states
            
            # Bipartite soft matching
            merge, _ = bipartite_soft_matching(
                metric,
                r,
                class_token=self._tome_info.get("class_token", True),
                distill_token=False,
            )
            
            # Weighted average merging
            hidden_states, self._tome_info["size"] = merge_wavg(
                merge,
                hidden_states,
                self._tome_info.get("size")
            )
            
            # ğŸ” DEBUG: æ‰“å°mergingç»“æœï¼ˆä»…layer 0, 5, 11, 23ï¼‰
            if self._layer_idx in [0, 5, 11, 23]:
                print(f"  [ToMe DEBUG] Layer {self._layer_idx}: {input_shape} â†’ {hidden_states.shape} (merged {r} tokens)")
        
        # 5. MLP
        residual = hidden_states
        hidden_states = self.layer_norm2(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        
        return (hidden_states,)


# ============================================================================
# ToMe Wrapperï¼šä¸»æ¨¡å‹åŒ…è£…å™¨
# ============================================================================

class VideoLLaVATomeWrapper(VideoLLaVAWrapper):
    """
    ToMeä¸“ç”¨Video-LLaVAåŒ…è£…å™¨
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½æ ‡å‡†Video-LLaVA-7Bæ¨¡å‹
    2. åŠ¨æ€æ›¿æ¢Vision Towerçš„CLIPEncoderLayerä¸ºToMeCLIPEncoderLayer
    3. é…ç½®merging schedule
    """
    
    def __init__(
        self,
        model_path: str = "/root/hhq/models/Video-LLaVA-7B-hf",
        token_budget: int = 2048,
        num_frames: int = 32
    ):
        """
        åˆå§‹åŒ–ToMe wrapper
        
        Args:
            model_path: Video-LLaVAæ¨¡å‹è·¯å¾„
            token_budget: tokené¢„ç®—ï¼ˆé»˜è®¤2048ï¼‰
            num_frames: é‡‡æ ·å¸§æ•°ï¼ˆé»˜è®¤32ï¼‰
        """
        # 1. è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆåŠ è½½æ ‡å‡†Video-LLaVAï¼‰
        print(f"[ToMe] Initializing Video-LLaVA with ToMe...")
        super().__init__(model_path)
        
        # â­ å¼ºåˆ¶è·å–vision tower
        # ğŸ” å…³é”®å‘ç°ï¼šVideo-LLaVAå®é™…ä½¿ç”¨IMAGE_TOWERå¤„ç†è§†é¢‘ï¼Œä¸æ˜¯video_towerï¼
        if self.vision_tower is None:
            print(f"[ToMe] âš ï¸ Vision tower not found in parent class, attempting direct access...")
            
            # VideoLlavaForConditionalGenerationç»“æ„ï¼š
            # self.model (VideoLlavaForConditionalGeneration)
            #   â””â”€â”€ .model (VideoLlavaModel)
            #       â”œâ”€â”€ .image_tower (CLIPVisionModel) â† â­ å®é™…ä½¿ç”¨è¿™ä¸ªï¼
            #       â””â”€â”€ .video_tower (CLIPVisionModel) â† âŒ ä¸ä½¿ç”¨è¿™ä¸ªï¼
            
            # â­ å…³é”®ä¿®å¤ï¼šä½¿ç”¨image_towerï¼ˆå®é™…è¢«è°ƒç”¨çš„towerï¼‰
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'image_tower'):
                self.vision_tower = self.model.model.image_tower
                print(f"[ToMe] âœ“ Found vision tower via model.model.image_tower (ACTUAL tower used!)")
            
            # Fallback: å¦‚æœæ²¡æœ‰image_towerï¼Œå°è¯•video_tower
            elif hasattr(self.model, 'model') and hasattr(self.model.model, 'video_tower'):
                self.vision_tower = self.model.model.video_tower
                print(f"[ToMe] âš ï¸ Using model.model.video_tower (fallback)")
            
            # ç¡®ä¿vision towerè¢«åˆå§‹åŒ–
            if self.vision_tower and hasattr(self.vision_tower, 'load_model'):
                self.vision_tower.load_model()
                print(f"[ToMe] âœ“ Vision tower initialized via load_model()")
        
        # éªŒè¯vision tower
        if self.vision_tower is None:
            # æ‰“å°æ¨¡å‹ç»“æ„å¸®åŠ©è°ƒè¯•
            print(f"[ToMe] âš ï¸ Detailed debugging info:")
            print(f"  - model type: {type(self.model)}")
            
            if hasattr(self.model, 'model'):
                print(f"  - model.model type: {type(self.model.model)}")
                
                # æ‰“å°æ‰€æœ‰å±æ€§
                all_attrs = [attr for attr in dir(self.model.model) if not attr.startswith('_')]
                print(f"  - model.model attributes (non-private): {all_attrs}")
                
                # æ£€æŸ¥_modules
                if hasattr(self.model.model, '_modules'):
                    print(f"  - model.model._modules keys: {list(self.model.model._modules.keys())}")
                
                # å°è¯•ç›´æ¥è®¿é—®å¯èƒ½çš„vision toweråç§°
                possible_names = ['vision_tower', 'vision_model', 'vision_encoder', 'image_encoder']
                for name in possible_names:
                    if hasattr(self.model.model, name):
                        attr = getattr(self.model.model, name)
                        print(f"  - âœ“ Found '{name}': {type(attr)}")
            
            raise RuntimeError(
                "[ToMe] âŒ Failed to load vision tower! "
                "Cannot find vision_tower in VideoLlavaForConditionalGeneration structure. "
                "See debug info above for available attributes."
            )
        
        print(f"[ToMe] âœ“ Vision tower loaded: {self.vision_tower.__class__.__name__}")
        
        # 2. ToMeé…ç½®
        self.token_budget = token_budget
        self.num_frames = num_frames
        
        # è®¡ç®—tokenæ•°ï¼š32å¸§ Ã— 256 tokens/å¸§ = 8192
        self.tokens_per_frame = 256  # Video-LLaVA-7B: 224x224 Ã· 14x14 = 256
        self.initial_tokens = num_frames * self.tokens_per_frame  # 8192
        
        # 3. è®¡ç®—merging schedule
        # Vision Towerå±‚æ•°ï¼šCLIP-ViT-Læœ‰24å±‚
        vision_encoder = self.vision_tower.vision_model.encoder
        self.num_layers = len(vision_encoder.layers)
        
        # Total tokens to merge
        total_to_merge = self.initial_tokens - self.token_budget  # 8192 - 2048 = 6144
        
        # Constant schedule: r_per_layer
        self.r_per_layer = max(1, total_to_merge // self.num_layers)  # 6144 / 24 = 256
        
        print(f"[ToMe] Configuration:")
        print(f"  - Num frames: {num_frames}")
        print(f"  - Tokens per frame: {self.tokens_per_frame}")
        print(f"  - Initial tokens: {self.initial_tokens}")
        print(f"  - Token budget: {token_budget}")
        print(f"  - Vision layers: {self.num_layers}")
        print(f"  - Tokens to merge: {total_to_merge}")
        print(f"  - Tokens per layer (r): {self.r_per_layer}")
        
        # 4. â­ æ³¨å…¥ToMeåˆ°Vision Tower
        self._inject_tome_to_vision_tower()
        
        print(f"[ToMe] âœ… Initialization complete")
    
    def _inject_tome_to_vision_tower(self):
        """
        åŠ¨æ€æ›¿æ¢Vision Towerçš„CLIPEncoderLayer
        
        å‚ç…§å®˜æ–¹tome/patch/timm.py:apply_patch (line 116-151)
        
        âš ï¸ å…³é”®ï¼šå¿…é¡»ä¿®æ”¹model.model.video_towerï¼Œè€Œä¸æ˜¯self.vision_towerï¼
        å› ä¸ºmodel.generate()ç›´æ¥è®¿é—®model.model.video_tower
        """
        print(f"[ToMe] Injecting ToMe into Vision Tower...")
        
        # ToMeé…ç½®å­—å…¸ï¼ˆæ‰€æœ‰å±‚å…±äº«ï¼‰
        self._tome_info = {
            "r": parse_r(self.num_layers, self.r_per_layer),  # [256, 256, ..., 256]
            "size": None,  # Token size tracking
            "class_token": True,  # CLIPæœ‰CLS token
            "distill_token": False,
        }
        
        # â­ å…³é”®ä¿®å¤ï¼šè·å–modelå®é™…ä½¿ç”¨çš„IMAGE_TOWER
        # æµ‹è¯•è¯æ˜ï¼šVideo-LLaVAå¤„ç†è§†é¢‘æ—¶è°ƒç”¨image_towerï¼Œä¸æ˜¯video_towerï¼
        if hasattr(self.model, 'model') and hasattr(self.model.model, 'image_tower'):
            actual_tower = self.model.model.image_tower
            print(f"  â†’ Using model.model.image_tower (VERIFIED: actual tower used during generation)")
        elif hasattr(self.model, 'model') and hasattr(self.model.model, 'video_tower'):
            actual_tower = self.model.model.video_tower
            print(f"  â†’ Fallback to model.model.video_tower")
        else:
            actual_tower = self.vision_tower
            print(f"  â†’ Fallback to self.vision_tower")
        
        # è·å–vision encoder
        vision_encoder = actual_tower.vision_model.encoder
        
        # éå†æ‰€æœ‰å±‚ï¼Œæ›¿æ¢class
        for layer_idx, layer in enumerate(vision_encoder.layers):
            # â­ åŠ¨æ€æ›¿æ¢classï¼ˆmonkey patchingæ ¸å¿ƒï¼‰
            original_class_name = layer.__class__.__name__
            
            # åˆ›å»ºToMeç‰ˆæœ¬çš„layerï¼ˆä¼ é€’layer_idxï¼‰
            tome_layer = ToMeCLIPEncoderLayer(layer, self._tome_info, layer_idx)
            
            # æ›¿æ¢åŸlayer
            vision_encoder.layers[layer_idx] = tome_layer
            
            if layer_idx == 0:
                print(f"  âœ“ Layer  {layer_idx}: {original_class_name} â†’ ToMeCLIPEncoderLayer")
        
        print(f"  âœ“ Replaced {self.num_layers} layers in {type(actual_tower).__name__}")
        print(f"[ToMe] Injection complete")
    
    def generate(self, video_tensor, prompt, options=None):
        """
        æ¨ç†æ¥å£ï¼ˆç»§æ‰¿çˆ¶ç±»ï¼Œæ— éœ€ä¿®æ”¹ï¼‰
        
        ToMeåœ¨Vision Towerå†…éƒ¨é€æ˜å·¥ä½œï¼Œä¸å½±å“å¤–éƒ¨æ¥å£
        """
        # Reset ToMe info for each forward pass
        self._tome_info["r"] = parse_r(self.num_layers, self.r_per_layer)
        self._tome_info["size"] = None
        
        # è°ƒç”¨çˆ¶ç±»generate
        return super().generate(video_tensor, prompt, options)


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("Testing ToMe Implementation")
    print("=" * 80)
    
    # æµ‹è¯•bipartite_soft_matching
    print("\nğŸ§ª Testing bipartite_soft_matching:")
    
    batch_size = 1
    num_tokens = 100
    channels = 512
    r = 10
    
    metric = torch.randn(batch_size, num_tokens, channels)
    merge, unmerge = bipartite_soft_matching(metric, r, class_token=True)
    
    x = torch.randn(batch_size, num_tokens, channels)
    merged = merge(x)
    
    print(f"  Input shape: {x.shape}")
    print(f"  Merged shape: {merged.shape}")
    print(f"  Expected: ({batch_size}, {num_tokens - r}, {channels})")
    
    assert merged.shape == (batch_size, num_tokens - r, channels), "Merge failed!"
    
    # æµ‹è¯•unmerge
    restored = unmerge(merged)
    assert restored.shape == x.shape, "Unmerge failed!"
    
    print(f"  âœ“ Bipartite matching works correctly")
    
    # æµ‹è¯•merge_wavg
    print("\nğŸ§ª Testing merge_wavg:")
    size = torch.ones(batch_size, num_tokens, 1)
    merged_wavg, merged_size = merge_wavg(merge, x, size)
    
    print(f"  Merged (wavg) shape: {merged_wavg.shape}")
    print(f"  Merged size shape: {merged_size.shape}")
    print(f"  âœ“ Weighted average merge works correctly")
    
    print("\nâœ… All tests passed!")
    print("=" * 80)
