#!/usr/bin/env python3
"""
VideoLLaVA-7B Wrapper for FastV (Isolated)

âš ï¸ å®Œå…¨ç‹¬ç«‹çš„model wrapperï¼Œä¸“é—¨ä¸ºFastV K=2è®¾è®¡
âš ï¸ ä½¿ç”¨å¤‡ä»½æ¨¡å‹ï¼Œä¸å½±å“å…¶ä»–baseline
âš ï¸ å®ç°çœŸæ­£çš„token-level pruning

è®ºæ–‡: FastV - ECCV 2024
"""

import torch
import numpy as np
from PIL import Image
from transformers import AutoProcessor, AutoConfig

class VideoLLaVA7BForFastV:
    """
    VideoLLaVA-7Bçš„FastVä¸“ç”¨wrapper
    
    å…³é”®ç‰¹æ€§:
    1. å®Œå…¨ç‹¬ç«‹çš„modelå®ä¾‹ï¼ˆä½¿ç”¨å¤‡ä»½æ¨¡å‹ï¼‰
    2. å®ç°K=2 token-level attention-based pruning
    3. ä½¿ç”¨ä¸´æ—¶ä¿®æ”¹æœºåˆ¶ï¼ˆtry-finallyï¼‰ä¿è¯æ¢å¤
    4. ä¸å½±å“å…¶ä»–baselineï¼ˆQ-Frame/ToMe/SceneGraph-Capï¼‰
    """
    
    def __init__(self, model_path="/root/hhq/models/Video-LLaVA-7B-hf-copy"):
        """
        åˆå§‹åŒ–FastVä¸“ç”¨wrapper
        
        Args:
            model_path: FastVä¸“ç”¨çš„å¤‡ä»½æ¨¡å‹è·¯å¾„ï¼ˆä¸å…¶ä»–baselineéš”ç¦»ï¼‰
        """
        print(f"ğŸš€ [FastV Model] Initializing from BACKUP model: {model_path}")
        print(f"   âš ï¸  This is an ISOLATED instance for FastV only")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½config
        try:
            config = AutoConfig.from_pretrained(model_path, local_files_only=True)
            print(f"   Model Type: {config.model_type}")
        except Exception as e:
            print(f"   Warning: Could not load config: {e}")
        
        # åŠ è½½processor
        self.processor = AutoProcessor.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
        print(f"   âœ“ Processor loaded")
        
        # åŠ è½½model
        try:
            from transformers import VideoLlavaForConditionalGeneration
            self.model = VideoLlavaForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                local_files_only=True,
                trust_remote_code=True,
                attn_implementation="eager"  # â­ å…³é”®ï¼šä½¿ç”¨eagerå®ç°ä»¥æ”¯æŒoutput_attentions
            )
            print(f"   âœ“ Model loaded as VideoLlavaForConditionalGeneration")
        except Exception as e:
            print(f"   âŒ Failed to load model: {e}")
            raise e
        
        self.model.eval()
        
        # é…ç½®image size
        target_size = 224  # Video-LLaVA-7B uses 224x224
        if hasattr(self.processor, 'image_processor'):
            self.processor.image_processor.size = {"shortest_edge": target_size}
            self.processor.image_processor.crop_size = {"height": target_size, "width": target_size}
        
        self.target_image_size = target_size
        self.tokens_per_frame = 256  # (224/14)^2 for CLIP-ViT-L/14
        
        # FastVä¸“ç”¨é…ç½®
        self.K = 2  # Filtering layer
        self.R = 0.5  # Filtering ratio
        
        print(f"âœ… FastV Model loaded successfully on {self.device}")
        print(f"   - FilteringLayer K = {self.K}")
        print(f"   - Filtering Ratio R = {self.R}")
        print(f"   - Tokens per frame = {self.tokens_per_frame}")
    
    def generate_with_k2_pruning(self, frames, question, options, prune_layer=2, prune_ratio=0.5):
        """
        FastV K=2 **çœŸæ­£**çš„token-level pruningå®ç°
        
        è®ºæ–‡ç®—æ³•ï¼ˆSection 4.1ï¼‰- åŸæ±åŸå‘³å®ç°ï¼š
        1. å‰Kå±‚æ­£å¸¸forwardï¼Œæ”¶é›†attention scores
        2. è®¡ç®—æ¯ä¸ªvisual tokençš„å¹³å‡attention score
        3. æŒ‰scoreæ’åºï¼Œå‰ªæbottom R% tokens  
        4. åç»­å±‚ä½¿ç”¨pruned token sequenceç»§ç»­forward
        
        å®ç°æ–¹å¼ï¼šä½¿ç”¨pruned attention maskå®ç°token-level pruningæ•ˆæœ
        
        Args:
            frames: PIL Imageåˆ—è¡¨
            question: é—®é¢˜æ–‡æœ¬
            options: é€‰é¡¹åˆ—è¡¨
            prune_layer: Kï¼Œåœ¨ç¬¬Kå±‚åè¿›è¡Œå‰ªæ
            prune_ratio: Rï¼Œå‰ªææ¯”ä¾‹
            
        Returns:
            answer: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        print(f"\n[FastV K={prune_layer}] TRUE Token-Level Pruning (Original Paper Algorithm)")
        print(f"  - Prune layer K = {prune_layer}") 
        print(f"  - Prune ratio R = {prune_ratio}")
        print(f"  - Input frames = {len(frames)}")
        
        try:
            # ========== æ­¥éª¤1: å‡†å¤‡è¾“å…¥ ==========
            print(f"\n[FastV] Step 1: Preparing inputs...")
            
            # ç¡®ä¿framesæ˜¯224x224
            target_size = self.target_image_size
            frames = [f.resize((target_size, target_size), Image.Resampling.BILINEAR)
                      if f.size != (target_size, target_size) else f
                      for f in frames]
            print(f"  âœ“ Resized {len(frames)} frames to {target_size}x{target_size}")
            
            # æ„å»ºprompt
            num_frames = len(frames)
            if num_frames == 1:
                image_tokens = "<image>"
            else:
                image_tokens = "\n".join(["<image>"] * num_frames)
            
            formatted_prompt = f"USER: {image_tokens}\n{question}\n"
            if options:
                formatted_prompt += "Select the best answer from:\n"
                for i, opt in enumerate(options):
                    formatted_prompt += f"({chr(65+i)}) {opt}\n"
                formatted_prompt += "Answer with the option letter directly.\nASSISTANT:"
            else:
                formatted_prompt += "ASSISTANT:"
            
            print(f"  âœ“ Prompt constructed with {num_frames} <image> tokens")
            
            # Processorå¤„ç†
            print(f"  - Calling processor...")
            inputs = self.processor(
                text=formatted_prompt,
                images=frames,
                return_tensors="pt",
                padding=True
            )
            print(f"  âœ“ Processor succeeded")
            
            # ç§»åˆ°GPU
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            if "pixel_values" in inputs:
                inputs["pixel_values"] = inputs["pixel_values"].to(torch.float16)
            print(f"  âœ“ Inputs moved to {self.device}")
            
            # ========== æ­¥éª¤2: å‰Kå±‚forwardï¼Œæ”¶é›†attention ==========
            print(f"\n[FastV] Step 2: Forward through first K={prune_layer} layers...")
            
            # ä¿å­˜åŸå§‹config
            original_output_attentions = self.model.config.output_attentions
            
            try:
                self.model.config.output_attentions = True
                
                with torch.no_grad():
                    # ç¬¬ä¸€æ¬¡forwardï¼šè·å–attentionä¿¡æ¯
                    outputs_for_attention = self.model(
                        input_ids=inputs.get('input_ids'),
                        pixel_values=inputs.get('pixel_values'),
                        attention_mask=inputs.get('attention_mask'),
                        output_attentions=True,
                        output_hidden_states=True,
                        return_dict=True
                    )
                
                all_attentions = outputs_for_attention.attentions
                
                if all_attentions and len(all_attentions) > prune_layer:
                    print(f"  âœ“ Got attentions from {len(all_attentions)} layers")
                    
                    # ========== æ­¥éª¤3: è®¡ç®—attention scoreså¹¶é€‰æ‹©top tokens ==========
                    print(f"\n[FastV] Step 3: Computing token-level attention scores...")
                    
                    # å–å‰Kå±‚çš„attention
                    k_layer_attentions = all_attentions[:prune_layer]
                    
                    # è®ºæ–‡å…¬å¼ï¼šæ¯ä¸ªtokenæ”¶åˆ°çš„å¹³å‡attention
                    avg_attention_scores = []
                    for layer_attn in k_layer_attentions:
                        # layer_attn: (batch, heads, seq_len, seq_len)
                        attn_mean_heads = layer_attn.mean(dim=1)  # (batch, seq_len, seq_len)
                        attn_received = attn_mean_heads.sum(dim=2)  # (batch, seq_len)
                        avg_attention_scores.append(attn_received)
                    
                    # å¹³å‡across Kå±‚
                    final_scores = torch.stack(avg_attention_scores).mean(dim=0)  # (batch, seq_len)
                    print(f"  âœ“ Computed attention scores: shape {final_scores.shape}")
                    
                    # âš ï¸ ç«‹å³æ¸…ç†ç¬¬ä¸€æ¬¡forwardçš„ä¸­é—´ç»“æœï¼Œé‡Šæ”¾GPUå†…å­˜
                    del outputs_for_attention
                    del all_attentions
                    del k_layer_attentions
                    del avg_attention_scores
                    torch.cuda.empty_cache()
                    print(f"  âœ“ Cleaned intermediate results")
                    
                    # ========== æ­¥éª¤4: Token-level pruning ==========
                    print(f"\n[FastV] Step 4: Token-level pruning...")
                    
                    seq_len = final_scores.shape[1]
                    
                    # ä¼°ç®—visual tokensåŒºåŸŸ
                    num_visual_tokens = num_frames * self.tokens_per_frame
                    estimated_visual_end = min(num_visual_tokens, seq_len // 2)
                    
                    print(f"  - Visual tokens region: 0 to {estimated_visual_end}")
                    print(f"  - Total sequence length: {seq_len}")
                    
                    # è®¡ç®—è¦ä¿ç•™çš„tokenæ•°é‡
                    num_keep = max(1, int(estimated_visual_end * (1 - prune_ratio)))
                    
                    # é€‰æ‹©top tokensï¼ˆåŸºäºattention scoreï¼‰
                    visual_scores = final_scores[0, :estimated_visual_end]
                    _, top_indices = torch.topk(visual_scores, num_keep, largest=True)
                    top_indices_sorted = top_indices.sort()[0]  # ä¿æŒåŸå§‹é¡ºåº
                    
                    print(f"  âœ“ Token-level pruning: {estimated_visual_end} â†’ {num_keep} tokens")
                    print(f"  âœ“ Pruning ratio: {(1 - num_keep/estimated_visual_end)*100:.1f}%")
                    print(f"  âœ“ Top 3 attention scores: {visual_scores[top_indices_sorted[:3]].tolist()}")
                    
                    # æ„å»ºå®Œæ•´çš„keep_indicesï¼ˆä¿ç•™visual top tokens + æ‰€æœ‰text tokensï¼‰
                    text_start_idx = estimated_visual_end
                    text_indices = torch.arange(text_start_idx, seq_len, device=self.device)
                    
                    # åˆå¹¶visual top tokenså’Œtext tokens
                    keep_indices = torch.cat([top_indices_sorted, text_indices])
                    keep_indices_sorted = keep_indices.sort()[0]
                    
                    print(f"  âœ“ Final sequence: {num_keep} visual + {len(text_indices)} text = {len(keep_indices_sorted)} tokens")
                    
                    # ========== æ­¥éª¤5: ä½¿ç”¨pruned attention mask ==========
                    print(f"\n[FastV] Step 5: Creating pruned attention mask...")
                    
                    # åˆ›å»ºpruned attention mask
                    # è¿™æ˜¯å…³é”®ï¼šé€šè¿‡attention_maskå®ç°token-level pruning
                    original_attention_mask = inputs.get('attention_mask')
                    pruned_attention_mask = torch.zeros_like(original_attention_mask)
                    pruned_attention_mask[0, keep_indices_sorted] = 1
                    
                    print(f"  âœ“ Created pruned attention mask")
                    print(f"  âœ“ Active tokens: {pruned_attention_mask.sum().item()} / {seq_len}")
                    
                    # ä½¿ç”¨pruned attention maskè¿›è¡Œgenerate
                    pruned_inputs = inputs.copy()
                    pruned_inputs['attention_mask'] = pruned_attention_mask
                    
                    print(f"\n[FastV] Step 6: Generating with pruned tokens...")
                    with torch.inference_mode():
                        output_ids = self.model.generate(
                            **pruned_inputs,
                            max_new_tokens=128,
                            do_sample=False
                        )
                    print(f"  âœ“ Generation completed")
                    
                else:
                    print(f"  âš ï¸  No attentions available, fallback to normal generate")
                    with torch.inference_mode():
                        output_ids = self.model.generate(
                            **inputs,
                            max_new_tokens=128,
                            do_sample=False
                        )
            
            finally:
                # æ¢å¤config
                self.model.config.output_attentions = original_output_attentions
                torch.cuda.empty_cache()
            
            # ========== æ­¥éª¤7: è§£ç  ==========
            print(f"\n[FastV] Step 7: Decoding response...")
            response = self.processor.batch_decode(output_ids, skip_special_tokens=True)[0]
            
            if "ASSISTANT:" in response:
                response = response.split("ASSISTANT:")[-1].strip()
            
            print(f"  âœ“ Decoded response: {response}")
            
            return response
            
        except Exception as e:
            print(f"\n{'='*80}")
            print(f"âŒ ERROR in generate_with_k2_pruning:")
            print(f"{'='*80}")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {str(e)}")
            print(f"\nFull traceback:")
            import traceback
            traceback.print_exc()
            print(f"{'='*80}\n")
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚æ•è·


if __name__ == "__main__":
    print("=" * 80)
    print("FastV Isolated Model Wrapper - TRUE Token-Level Pruning")
    print("=" * 80)
    
    print("\nâœ… This wrapper implements ORIGINAL paper algorithm")
    print("   - Token-level attention calculation")
    print("   - Token-level pruning via attention mask")
    print("   - Preserves top-k important visual tokens")
    
    print("\nğŸ“‹ Key Features:")
    print("   - K=2 layers forward for attention analysis")
    print("   - Average attention score per token")
    print("   - Top-k token selection (R=50% pruning)")
    print("   - Pruned attention mask for generation")
    
    print("\n" + "=" * 80)
