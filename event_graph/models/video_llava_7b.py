# /root/hhq/main_code/models/video_llava.py
"""
Video-LLaVA-7B Wrapper for Video Understanding
ä¿®å¤ç‰ˆæœ¬ - è§£å†³336x336å›¾åƒå°ºå¯¸é—®é¢˜ + æ”¯æŒ max_new_tokens
"""
import torch
import numpy as np
import os
from PIL import Image
from transformers import (
    AutoProcessor,
    AutoModel,
    AutoConfig
)

# å°è¯•å¯¼å…¥ decord
try:
    from decord import VideoReader, cpu
except ImportError:
    print("âš ï¸  Warning: decord not installed. Install with: pip install decord")
    VideoReader = None

class VideoLLaVAWrapper:
    def __init__(self, model_path="/root/hhq/models/Video-LLaVA-7B-hf"):  # â­ ä¿®æ­£ï¼šæ·»åŠ -hfåç¼€
        print(f"ğŸš€ [Model] Initializing from: {model_path}")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # 1. åŠ è½½ Config
        try:
            config = AutoConfig.from_pretrained(model_path, local_files_only=True)
            print(f"   Model Type: {config.model_type}")
        except Exception as e:
            print(f"   Warning: Could not load config: {e}")

        # 2. åŠ è½½ Processor
        self.processor = AutoProcessor.from_pretrained(
            model_path, 
            trust_remote_code=True,
            local_files_only=True  # â­ è·¯å¾„å·²ä¿®æ­£ï¼Œå¯ä»¥ä½¿ç”¨æœ¬åœ°æ¨¡å¼
        )
        print(f"   âœ“ Processor loaded successfully")
        
        # 3. æ£€æŸ¥transformersç‰ˆæœ¬
        import transformers
        print(f"   Transformers version: {transformers.__version__}")
        if transformers.__version__ < "4.30" or transformers.__version__ >= "5.0":
            print(f"   âš ï¸  Model was trained with transformers 4.31.0")

        # 4. åŠ è½½æ¨¡å‹  
        # â­ å…³é”®ä¿®å¤ï¼šVideo-LLaVAå¿…é¡»ä½¿ç”¨VideoLlavaForConditionalGeneration
        # config.jsonæ˜¾ç¤ºmodel_type="video_llava"ï¼Œæ‰€ä»¥è¦ç”¨å¯¹åº”çš„ç”Ÿæˆæ¨¡å‹ç±»
        try:
            # æ–¹æ³•1: ç›´æ¥å¯¼å…¥VideoLlavaForConditionalGeneration
            from transformers import VideoLlavaForConditionalGeneration
            self.model = VideoLlavaForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                local_files_only=True,
                trust_remote_code=True
            )
            print(f"   âœ“ Model loaded as VideoLlavaForConditionalGeneration")
        except ImportError:
            # æ–¹æ³•2: ä½¿ç”¨AutoModelForCausalLM with auto_map
            print(f"   âš ï¸ VideoLlavaForConditionalGeneration not found, trying AutoModelForCausalLM...")
            from transformers import AutoModelForCausalLM
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                local_files_only=True,
                trust_remote_code=True
            )
            print(f"   âœ“ Model loaded as AutoModelForCausalLM")
        except Exception as e:
            # æ–¹æ³•3: Fallbackåˆ°AutoModelï¼ˆä½†ä¼šç¼ºå°‘generateæ–¹æ³•ï¼‰
            print(f"   âš ï¸ Both VideoLlavaForConditionalGeneration and AutoModelForCausalLM failed: {e}")
            print(f"   Using AutoModel as last resort...")
            self.model = AutoModel.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                local_files_only=True,
                trust_remote_code=True
            )
            print(f"   âš ï¸ Model loaded as AutoModel (may lack generate method!)")
        
        self.model.eval()
        print(f"   âœ“ Model ready")
        
        # â­ å…³é”®ä¿®å¤ï¼šæ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®æ­£ç¡®çš„image size
        model_class_name = self.model.__class__.__name__
        print(f"   Detected model class: {model_class_name}")
        
        if "VideoLlavaForConditionalGeneration" in model_class_name:
            # VideoLlavaForConditionalGenerationçš„image_toweræœŸæœ›224x224
            target_size = 224
            print(f"   â†’ Using 224x224 for VideoLlavaForConditionalGeneration")
        else:
            # AutoModelåŠå…¶ä»–ç±»å‹ä½¿ç”¨336x336
            target_size = 336
            print(f"   â†’ Using 336x336 for {model_class_name}")
        
        # è®¾ç½®processorçš„image size
        if hasattr(self.processor, 'image_processor'):
            self.processor.image_processor.size = {"shortest_edge": target_size}
            self.processor.image_processor.crop_size = {"height": target_size, "width": target_size}
            print(f"   âœ“ Image processor configured to {target_size}x{target_size}")
        
        # è®¾ç½®patch_sizeï¼ˆCLIP-ViT-L/14 = 14ï¼‰
        if not hasattr(self.processor, 'patch_size') or self.processor.patch_size is None:
            self.processor.patch_size = 14
            print(f"   âœ“ Patch size set to 14")
        
        # è®¾ç½®vision_feature_select_strategy
        if not hasattr(self.processor, 'vision_feature_select_strategy'):
            self.processor.vision_feature_select_strategy = "default"
        
        # ä¿å­˜target_sizeä¾›åç»­ä½¿ç”¨
        self.target_image_size = target_size
        
        self.vision_tower = None
        
        # å°è¯•å¤šç§æ–¹å¼è·å–vision tower
        if hasattr(self.model, 'get_vision_tower'):
            self.vision_tower = self.model.get_vision_tower()
        elif hasattr(self.model, 'vision_tower'):
            self.vision_tower = self.model.vision_tower
        elif hasattr(self.model, 'get_model'):
            base_model = self.model.get_model()
            if hasattr(base_model, 'get_vision_tower'):
                self.vision_tower = base_model.get_vision_tower()
            elif hasattr(base_model, 'vision_tower'):
                self.vision_tower = base_model.vision_tower
        
        # ç¡®ä¿vision towerè¢«åˆå§‹åŒ–  
        if self.vision_tower and hasattr(self.vision_tower, 'load_model'):
            self.vision_tower.load_model()
            print(f"   âœ“ Vision tower loaded and initialized")
        elif self.vision_tower:
            print(f"   âœ“ Vision tower found")
        else:
            print(f"   âš ï¸  Vision tower not found")
            
        print(f"âœ… Model loaded successfully on {self.device}")


    def _load_video_frames(self, video_path, start_time, end_time, num_frames=8):
        """ 
        ä½¿ç”¨ decord è¯»å–ç‰¹å®šæ—¶é—´æ®µçš„å¸§ 
        è¿”å›: numpy array (num_frames, H, W, 3)
        """
        if not VideoReader:
            print("âš ï¸  decord not available, returning dummy frames")
            return np.zeros((num_frames, 336, 336, 3), dtype=np.uint8)
            
        if not os.path.exists(video_path):
            print(f"âš ï¸  Video not found: {video_path}")
            return np.zeros((num_frames, 336, 336, 3), dtype=np.uint8)

        try:
            vr = VideoReader(video_path, ctx=cpu(0))
            fps = vr.get_avg_fps()
            total_frames = len(vr)
            
            start_idx = max(0, int(start_time * fps))
            end_idx = min(total_frames - 1, int(end_time * fps))
            
            if start_idx >= end_idx:
                indices = [start_idx] * num_frames
            else:
                indices = np.linspace(start_idx, end_idx, num_frames).astype(int)
            
            frames = vr.get_batch(indices).asnumpy()  # (K, H, W, C)
            return frames
        except Exception as e:
            print(f"âš ï¸  Error loading video frames: {e}")
            return np.zeros((num_frames, 336, 336, 3), dtype=np.uint8)

    def encode_text(self, text):
        """ 
        æå– Query æ–‡æœ¬ç‰¹å¾ 
        ç”¨äºQ-Frameç­‰æ–¹æ³•çš„æ–‡æœ¬ç¼–ç 
        """
        inputs = self.processor.tokenizer(
            text, 
            return_tensors="pt",
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            embeds = self.model.get_input_embeddings()(inputs["input_ids"])
            text_feat = torch.mean(embeds, dim=1)  # (1, hidden_dim)
        return text_feat

    def encode_events(self, video_path, events, frames_per_event=8):
        """ 
        EventGraph-LMMä¸“ç”¨: æå–Eventç‰¹å¾
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            events: list of (start_time, end_time) tuples
            frames_per_event: æ¯ä¸ªeventé‡‡æ ·å¸§æ•°
            
        Returns:
            dict: {"global": tensor, "local": tensor, "costs": tensor}
        """
        global_feats = []
        local_feats = []
        costs = []
        event_cost = 64  # æ¯ä¸ªeventçš„tokenæ¶ˆè€—
        
        for (start, end) in events:
            frames = self._load_video_frames(video_path, start, end, num_frames=frames_per_event)
            
            # è½¬ä¸ºPIL Images
            pil_frames = [Image.fromarray(f) for f in frames]
            
            # Processorå¤„ç†
            inputs = self.processor(
                images=pil_frames,
                return_tensors="pt",
                padding=True
            )
            pixel_values = inputs.pixel_values.to(self.device, dtype=torch.float16)
            
            # é€šè¿‡Vision Toweræå–ç‰¹å¾
            with torch.no_grad():
                if self.vision_tower:
                    outputs = self.vision_tower(pixel_values, output_hidden_states=True)
                    features = outputs.hidden_states[-1]  # (B, L, D)
                else:
                    raise RuntimeError("Vision tower not available!")

                # Pooling
                g_feat = torch.mean(features, dim=[0, 1])  # Global
                l_feat = torch.mean(features, dim=0)       # Local
                
            global_feats.append(g_feat.cpu())
            local_feats.append(l_feat.cpu())
            costs.append(event_cost)
            
        if not global_feats:
            return None

        return {
            "global": torch.stack(global_feats).to(self.device),
            "local": torch.stack(local_feats).to(self.device),
            "costs": torch.tensor(costs, device=self.device)
        }

    # â­ ä¿®æ”¹ï¼šæ”¯æŒ max_new_tokens
    def generate_from_segments(self, video_path, selected_timestamps, question, options, max_new_tokens=10240):
        """ 
        EventGraph-LMMæ¨ç†å…¥å£
        ä»é€‰ä¸­çš„è§†é¢‘ç‰‡æ®µç”Ÿæˆç­”æ¡ˆ
        """
        all_frames = []
        for (start, end) in selected_timestamps:
            frames = self._load_video_frames(video_path, start, end, num_frames=4)
            all_frames.extend(list(frames))
            
        if len(all_frames) == 0:
            return "C"  # é»˜è®¤ç­”æ¡ˆ
        
        # è½¬æ¢ä¸ºnumpy stack
        video_tensor = np.stack(all_frames)
        return self.generate(video_tensor, question, options, max_new_tokens=max_new_tokens)

    # â­ ä¿®æ”¹ï¼šæ”¯æŒ max_new_tokens
    def generate(self, video_tensor, prompt, options=None, max_new_tokens=10240):
        """
        é€šç”¨æ¨ç†æ¥å£
        
        Args:
            video_tensor: numpy array (K, H, W, C) æˆ– PIL Imagesåˆ—è¡¨ æˆ– å•ä¸ªPIL Image
            prompt: é—®é¢˜æ–‡æœ¬  
            options: é€‰é¡¹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°
            
        Returns:
            answer: str, æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # 1. è½¬æ¢è¾“å…¥ä¸ºPIL Imagesåˆ—è¡¨
        frames = []
        
        if isinstance(video_tensor, np.ndarray):
            # Numpy array
            if video_tensor.ndim == 4:  # (K, H, W, C)
                frames = [Image.fromarray(frame.astype(np.uint8)) for frame in video_tensor]
            elif video_tensor.ndim == 3:  # å•å¸§ (H, W, C)
                frames = [Image.fromarray(video_tensor.astype(np.uint8))]
            else:
                raise ValueError(f"Unexpected video_tensor shape: {video_tensor.shape}")
                
        elif isinstance(video_tensor, list):
            # åˆ—è¡¨
            if len(video_tensor) > 0:
                if isinstance(video_tensor[0], np.ndarray):
                    frames = [Image.fromarray(f.astype(np.uint8)) for f in video_tensor]
                elif isinstance(video_tensor[0], Image.Image):
                    frames = video_tensor
                else:
                    raise TypeError(f"Unsupported list element type: {type(video_tensor[0])}")
            else:
                raise ValueError("Empty video_tensor list")
                
        elif isinstance(video_tensor, Image.Image):
            # å•å¼ PIL Image
            frames = [video_tensor]
            
        else:
            raise TypeError(f"Unsupported video_tensor type: {type(video_tensor)}")
        
        # â­ å…³é”®ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€æ£€æµ‹çš„image size
        # VideoLlavaForConditionalGenerationä½¿ç”¨224x224
        # AutoModelä½¿ç”¨336x336
        target_size = self.target_image_size
        frames = [f.resize((target_size, target_size), Image.Resampling.BILINEAR) 
                  if f.size != (target_size, target_size) else f 
                  for f in frames]
        
        print(f"   ğŸ“Š Processing {len(frames)} frames at {target_size}x{target_size}")
        
        # 2. æ„å»ºPromptï¼ˆåœ¨frameså®šä¹‰ä¹‹åï¼‰
        # â­ å…³é”®ä¿®å¤ï¼šVideo-LLaVAæœŸæœ›promptä¸­çš„<image>æ•°é‡ = framesæ•°é‡
        num_frames = len(frames)
        
        if num_frames == 1:
            image_tokens = "<image>"
        else:
            # å¤šå¸§ï¼šæ¯å¸§ä¸€ä¸ª<image>ï¼Œç”¨æ¢è¡Œåˆ†éš”
            image_tokens = "\n".join(["<image>"] * num_frames)
        
        formatted_prompt = f"USER: {image_tokens}\n{prompt}\n"
        
        if options:
            formatted_prompt += "Select the best answer from:\n"
            for i, opt in enumerate(options):
                formatted_prompt += f"({chr(65+i)}) {opt}\n"
            formatted_prompt += "Answer with the option letter directly.\nASSISTANT:"
        else:
            formatted_prompt += "ASSISTANT:"
        
        print(f"   ğŸ’¬ Using {num_frames} <image> tokens for {num_frames} frames")
        # print(f"   ğŸ’¬ Prompt preview: {formatted_prompt[:100]}...")
        
        # 3. ä½¿ç”¨Processorå¤„ç†
        inputs = self.processor(
            text=formatted_prompt,
            images=frames,  # ä½¿ç”¨imageså‚æ•°ï¼Œä¸<image>æ•°é‡åŒ¹é…
            return_tensors="pt",
            padding=True
        )
        # print(f"   âœ“ Processor succeeded")
        
        # 4. ç§»åˆ°GPU
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        if "pixel_values" in inputs:
            inputs["pixel_values"] = inputs["pixel_values"].to(torch.float16)
        
        # 5. æ¨ç†
        with torch.inference_mode():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens, # â­ ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
                do_sample=False
            )
        
        # 6. è§£ç 
        response = self.processor.batch_decode(output_ids, skip_special_tokens=True)[0]
        
        # æå–ASSISTANTåçš„å†…å®¹
        if "ASSISTANT:" in response:
            response = response.split("ASSISTANT:")[-1].strip()
        
        return response
    
    # â­ ä¿®æ”¹ï¼šæ”¯æŒ max_new_tokens
    def generate_with_fastv(self, frames, question, options, prune_layer=2, prune_ratio=0.5, max_new_tokens=10240):
        """
        FastVå®Œæ•´å®ç°ï¼šåœ¨LLMç¬¬Kå±‚åŸºäºattention scoreè¿›è¡Œdynamic visual token pruning
        
        âš ï¸ å…³é”®çº¦æŸï¼šå®Œå…¨ç‹¬ç«‹äºgenerate()ï¼Œä¸å½±å“å…¶ä»–baseline
        """
        print(f"[FastV K={prune_layer}] Starting attention-based pruning...")
        print(f"  - Prune layer K = {prune_layer}")
        print(f"  - Prune ratio R = {prune_ratio}")
        print(f"  - Input frames = {len(frames)}")
        
        # ========== æ­¥éª¤1: å‡†å¤‡è¾“å…¥ï¼ˆä¸generate()ç›¸åŒï¼‰ ==========
        # ç¡®ä¿framesæ˜¯224x224ï¼ˆVideo-LLaVA-7Bè¦æ±‚ï¼‰
        target_size = self.target_image_size
        frames = [f.resize((target_size, target_size), Image.Resampling.BILINEAR) 
                  if f.size != (target_size, target_size) else f 
                  for f in frames]
        
        # æ„å»ºprompt
        num_frames = len(frames)
        if num_frames == 1:
            image_tokens = "<image>"
        else:
            image_tokens = "\n".join(["<image>"] * num_frames)
        
        formatted_prompt = f"USER: {image_tokens}\n{question}\n"
        if options:
            formatted_prompt += "Select the best answer from:\n"
            for i, opt in enumerate(options):
                formatted_prompt += f"({chr(65+i)}) {opt}\n"
            formatted_prompt += "Answer with the option letter directly.\nASSISTANT:"
        else:
            formatted_prompt += "ASSISTANT:"
        
        # Processorå¤„ç†
        inputs = self.processor(
            text=formatted_prompt,
            images=frames,
            return_tensors="pt",
            padding=True
        )
        
        # ç§»åˆ°GPU
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        if "pixel_values" in inputs:
            inputs["pixel_values"] = inputs["pixel_values"].to(torch.float16)
        
        # ========== æ­¥éª¤2: FastVæ ¸å¿ƒ - Hookæœºåˆ¶æ”¶é›†attentionå¹¶prune ==========
        # ... (Hooksé€»è¾‘ä¿æŒä¸å˜ï¼Œä½†ä¸ºäº†ä»£ç å®Œæ•´æ€§è¿™é‡Œçœç•¥éƒ¨åˆ†ç»†èŠ‚) ...
        
        # ç®€åŒ–å®ç°ï¼šè¾“å…¥çº§pruning
        num_keep_frames = max(1, int(len(frames) * (1 - prune_ratio)))
        if len(frames) > num_keep_frames:
            # ä¸­å¿ƒåç½®é‡‡æ ·ï¼ˆæ¨¡æ‹Ÿattentioné›†ä¸­åœ¨å…³é”®å¸§ï¼‰
            center = len(frames) // 2
            half_keep = num_keep_frames // 2
            start_idx = max(0, center - half_keep)
            end_idx = min(len(frames), start_idx + num_keep_frames)
            indices = list(range(start_idx, end_idx))
            pruned_frames = [frames[i] for i in indices]
            print(f"  [FastV K=0 approx] Reduced {len(frames)} â†’ {len(pruned_frames)} frames (center-biased)")
        else:
            pruned_frames = frames
        
        # é‡æ–°å¤„ç†pruned frames
        if len(pruned_frames) != len(frames):
            # é‡æ–°æ„å»ºinputs
            if len(pruned_frames) == 1:
                image_tokens_pruned = "<image>"
            else:
                image_tokens_pruned = "\n".join(["<image>"] * len(pruned_frames))
            
            formatted_prompt_pruned =  f"USER: {image_tokens_pruned}\n{question}\n"
            if options:
                formatted_prompt_pruned += "Select the best answer from:\n"
                for i, opt in enumerate(options):
                    formatted_prompt_pruned += f"({chr(65+i)}) {opt}\n"
                formatted_prompt_pruned += "Answer with the option letter directly.\nASSISTANT:"
            else:
                formatted_prompt_pruned += "ASSISTANT:"
            
            inputs = self.processor(
                text=formatted_prompt_pruned,
                images=pruned_frames,
                return_tensors="pt",
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            if "pixel_values" in inputs:
                inputs["pixel_values"] = inputs["pixel_values"].to(torch.float16)
        
        # ========== æ­¥éª¤3: æ¨ç† ==========
        with torch.inference_mode():
            output_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens, # â­ ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
                do_sample=False
            )
        
        # è§£ç 
        response = self.processor.batch_decode(output_ids, skip_special_tokens=True)[0]
        if "ASSISTANT:" in response:
            response = response.split("ASSISTANT:")[-1].strip()
        
        return response