# # # /root/icml2026/event_graph/code/eventgraph.py
# # import torch
# # import numpy as np
# # import cv2  # <--- ä¿®å¤æŠ¥é”™: ä¹‹å‰ç¼ºå°‘è¿™ä¸ªå¯¼å…¥
# # import os
# # from PIL import Image
# # from transformers import CLIPProcessor, CLIPModel
# # from .base_method import BaseMethod
# # from .graph_builder import compute_similarity_matrix, compute_pagerank_matrix
# # from .celf_solver import CELFSelector
# # from .uboco_detector import UbocoDetector

# # try:
# #     from decord import VideoReader, cpu
# # except ImportError:
# #     print("âš ï¸ Warning: decord not installed")
# #     VideoReader = None

# # class EventGraphLMM(BaseMethod):
# #     def __init__(self, args, model):
# #         super().__init__(args, model)
        
# #         # Params from Paper Section 4.1
# #         self.tau = 30.0  
# #         self.delta = 0.65 
# #         self.alpha = 0.15 
# #         self.lambda_param = 1.0 
# #         self.token_budget = args.token_budget
        
# #         # Detect token density for budget calculation
# #         backbone_name = getattr(args, 'backbone', '')
# #         if '34B' in backbone_name:
# #             self.tokens_per_frame = 576 
# #         elif 'Qwen' in backbone_name:
# #             self.tokens_per_frame = 256 
# #         else:
# #             self.tokens_per_frame = 256 # Video-LLaVA-7B default
            
# #         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
# #         # 1. Load CLIP Once
# #         self._load_clip_model()
        
# #         # 2. Initialize Uboco with the SHARED model
# #         self.shot_detector = UbocoDetector(
# #             device=self.device, 
# #             clip_model=self.clip_model, 
# #             clip_processor=self.clip_processor
# #         )

# #     def _load_clip_model(self):
# #         # Uses local path if available to save download time
# #         local_path = "/root/hhq/models/clip-vit-large-patch14"
# #         model_name = local_path if os.path.exists(local_path) else "openai/clip-vit-large-patch14"
        
# #         try:
# #             self.clip_processor = CLIPProcessor.from_pretrained(model_name)
# #             self.clip_model = CLIPModel.from_pretrained(model_name).to(self.device)
# #             self.clip_model.eval()
# #         except Exception as e:
# #             print(f"Warning: Loading CLIP from {model_name} failed ({e}), trying openai default.")
# #             self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
# #             self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
# #             self.clip_model.eval()

# #     def _detect_shot_boundaries(self, video_path):
# #         # Delegate to the shared-model Uboco detector
# #         try:
# #             # Uboco return timestamps (e.g., [2.5, 5.1, ...])
# #             boundaries = self.shot_detector.detect(video_path, sample_rate=2) 
# #             # Convert boundaries to event intervals (start, end)
# #             events = self._boundaries_to_events(boundaries, video_path)
            
# #             # Filter noise < 1s
# #             events = [e for e in events if (e[1] - e[0]) >= 1.0]
            
# #             # If too few events, use fallback
# #             if len(events) < 3: 
# #                 return self._fallback_windows(video_path)
# #             return events
# #         except Exception as e:
# #             print(f"  [EventGraph] Uboco failed ({e}), using fallback.")
# #             return self._fallback_windows(video_path)

# #     def _boundaries_to_events(self, boundaries, video_path):
# #         """
# #         ä¿®å¤æŠ¥é”™: å°†æ—¶é—´æˆ³åˆ—è¡¨è½¬æ¢ä¸º (start, end) å…ƒç»„åˆ—è¡¨
# #         """
# #         # è·å–è§†é¢‘æ€»æ—¶é•¿
# #         cap = cv2.VideoCapture(video_path)
# #         fps = cap.get(cv2.CAP_PROP_FPS)
# #         frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
# #         duration = frame_count / fps if fps > 0 else 0
# #         cap.release()

# #         if duration == 0: return []

# #         # æ„é€ åŒºé—´
# #         sorted_bounds = sorted([0.0] + list(boundaries) + [duration])
# #         # å»é‡
# #         sorted_bounds = sorted(list(set(sorted_bounds)))
        
# #         events = []
# #         for i in range(len(sorted_bounds) - 1):
# #             start = sorted_bounds[i]
# #             end = sorted_bounds[i+1]
# #             events.append((start, end))
# #         return events

# #     def _extract_event_features(self, video_path, events):
# #         # Batch processing for CLIP
# #         if VideoReader is None: raise ImportError("decord required")
# #         vr = VideoReader(video_path, ctx=cpu(0))
# #         fps = vr.get_avg_fps()
        
# #         representative_frames = []
# #         valid_indices = []
        
# #         for idx, (start_t, end_t) in enumerate(events):
# #             mid_t = (start_t + end_t) / 2.0
# #             # å®‰å…¨æ£€æŸ¥é˜²æ­¢è¶Šç•Œ
# #             if mid_t * fps >= len(vr): continue
            
# #             frame_idx = min(len(vr) - 1, int(mid_t * fps))
# #             frame_np = vr[frame_idx].asnumpy()
# #             representative_frames.append(Image.fromarray(frame_np))
# #             valid_indices.append(idx)
        
# #         # Batch Process
# #         batch_size = 32
# #         global_feats_list = []
# #         local_feats_list = []
        
# #         with torch.no_grad():
# #             for i in range(0, len(representative_frames), batch_size):
# #                 batch = representative_frames[i : i+batch_size]
# #                 inputs = self.clip_processor(images=batch, return_tensors="pt", padding=True)
# #                 inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
# #                 # Global (CLS)
# #                 g_feats = self.clip_model.get_image_features(**inputs)
# #                 global_feats_list.append(g_feats)
                
# #                 # Local (Patches)
# #                 outputs = self.clip_model.vision_model(**inputs, output_hidden_states=True)
# #                 l_feats = outputs.last_hidden_state[:, 1:, :] # Remove CLS
# #                 local_feats_list.append(l_feats)
        
# #         if len(global_feats_list) == 0:
# #             return torch.tensor([]), torch.tensor([]), []

# #         global_feats = torch.cat(global_feats_list, dim=0)
# #         local_feats = torch.cat(local_feats_list, dim=0)
        
# #         return global_feats, local_feats, representative_frames

# #     def _construct_event_graph(self, global_feats, local_feats, events):
# #         """
# #         æ„å»ºè¯­ä¹‰-æ—¶åºå›¾
# #         """
# #         N = global_feats.shape[0]
# #         # 1. Compute Semantic Adjacency (Eq. 3, 4)
# #         adj_semantic = compute_similarity_matrix(
# #             global_feats, local_feats, 
# #             tau=self.tau, 
# #             event_times=events, 
# #             threshold=self.delta
# #         )
# #         # 2. Compute Reachability (PageRank, Eq. 6)
# #         Pi = compute_pagerank_matrix(adj_semantic, alpha=self.alpha)
# #         return Pi

# #     def _select_subgraph(self, Pi, question, global_feats, events):
# #         """
# #         CELF ç®—æ³•é€‰æ‹©å…³é”®å­å›¾
# #         """
# #         # 1. Encode Question
# #         inputs = self.clip_processor(text=[question], return_tensors="pt", padding=True)
# #         inputs = {k: v.to(self.device) for k, v in inputs.items()}
# #         with torch.no_grad():
# #             q_feat = self.clip_model.get_text_features(**inputs)
# #             q_feat = q_feat / q_feat.norm(dim=-1, keepdim=True)
        
# #         # 2. Calculate Query Relevance (Eq. 5)
# #         g_norm = global_feats / global_feats.norm(dim=-1, keepdim=True)
# #         relevance = torch.mm(g_norm, q_feat.t()).squeeze() # (N,)
# #         relevance = torch.clamp(relevance, min=0.0) # ReLU
        
# #         # 3. Calculate Cost (Token consumption)
# #         # ç®€å•çš„çº¿æ€§ä»£ä»·: æ¯ä¸ªäº‹ä»¶æ¶ˆè€— tokens_per_frame
# #         costs = torch.full((len(events),), self.tokens_per_frame, device=self.device)
        
# #         # 4. CELF Selection
# #         selector = CELFSelector(Pi, relevance, costs, lambda_param=self.lambda_param)
# #         selected_indices = selector.select(budget=self.token_budget)
        
# #         return selected_indices

# #     def _fallback_windows(self, video_path):
# #         """
# #         å¦‚æœ Uboco å¤±è´¥ï¼Œä½¿ç”¨ç­‰é—´éš”åˆ‡ç‰‡
# #         """
# #         cap = cv2.VideoCapture(video_path)
# #         fps = cap.get(cv2.CAP_PROP_FPS)
# #         count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
# #         cap.release()
        
# #         duration = count / fps if fps > 0 else 0
# #         events = []
# #         # æ¯2ç§’åˆ‡ä¸€æ®µ
# #         step = 2.0
# #         for t in np.arange(0, duration, step):
# #             events.append((t, min(t + step, duration)))
        
# #         # å¦‚æœè§†é¢‘æçŸ­æˆ–è€…è¯»ä¸åˆ°ï¼Œç»™ä¸€ä¸ªé»˜è®¤
# #         if not events:
# #             events = [(0.0, 1.0)]
            
# #         return events

# #     def _build_graph_cot_prompt(self, question, options, segments, adj_matrix, selected_indices):
# #         event_timeline = [f"Event{i+1}" for i, _, _ in segments]
        
# #         # æ ¼å¼åŒ–é€‰é¡¹å­—ç¬¦ä¸²
# #         if isinstance(options, list):
# #             options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options)])
# #         else:
# #             options_str = str(options)

# #         prompt = (
# #             f"Question: {question}\n"
# #             f"Options:\n{options_str}\n"
# #             f"Key Events Timeline: {' -> '.join(event_timeline)}\n"
# #             f"Based on these visual events, reason step-by-step and choose the best answer."
# #         )
# #         return prompt

# #     def process_and_inference(self, video_path, question, options):
# #         # 1. æ£€æµ‹äº‹ä»¶
# #         events = self._detect_shot_boundaries(video_path)
# #         if not events: return "C"
        
# #         # 2. æå–ç‰¹å¾
# #         global_feats, local_feats, frames = self._extract_event_features(video_path, events)
# #         if len(frames) == 0: return "C"

# #         # 3. å»ºå›¾
# #         Pi = self._construct_event_graph(global_feats, local_feats, events)
        
# #         # 4. é€‰å›¾
# #         sel_idx = self._select_subgraph(Pi, question, global_feats, events)
# #         if not sel_idx: sel_idx = [0] # Fallback
        
# #         # 5. å‡†å¤‡æ¨ç†æ•°æ®
# #         selected_frames = [frames[i] for i in sorted(sel_idx)]
# #         selected_segments = [(events[i][0], events[i][1], i) for i in sorted(sel_idx)]
        
# #         # 6. ç”Ÿæˆ Prompt
# #         prompt = self._build_graph_cot_prompt(
# #             question, options, 
# #             selected_segments, 
# #             Pi, sel_idx
# #         )
        
# #         # 7. è°ƒç”¨ VLM æ¨ç†
# #         return self.model.generate(selected_frames, prompt, options)


# # /root/icml2026/event_graph/code/eventgraph.py
# import torch
# import numpy as np
# import cv2
# import os
# from PIL import Image
# from transformers import CLIPProcessor, CLIPModel
# from .base_method import BaseMethod
# from .graph_builder import compute_similarity_matrix, compute_pagerank_matrix
# from .celf_solver import CELFSelector

# # === ä¿®æ”¹ 1: å¯¼å…¥ TransNetV2Detector ===
# # å‡è®¾ä½ æŠŠä»£ç ä¿å­˜ä¸ºäº† transnet_detector.py
# try:
#     from .transnet_detector import TransNetV2Detector
# except ImportError:
#     print("âš ï¸ Warning: Could not import TransNetV2Detector. Make sure transnet_detector.py exists.")
#     TransNetV2Detector = None

# try:
#     from decord import VideoReader, cpu
# except ImportError:
#     print("âš ï¸ Warning: decord not installed")
#     VideoReader = None

# class EventGraphLMM(BaseMethod):
#     def __init__(self, args, model):
#         super().__init__(args, model)
        
#         # Params from Paper Section 4.1
#         self.tau = 30.0  
#         self.delta = 0.65 
#         self.alpha = 0.15 
#         self.lambda_param = 1.0 
#         self.token_budget = args.token_budget
        
#         # Detect token density for budget calculation
#         backbone_name = getattr(args, 'backbone', '')
#         if 'Qwen' in backbone_name:
#             # ç­–ç•¥ A: å¼ºåˆ¶ Resize åˆ° 336x336 (æ¨è) -> Token æ¶ˆè€—ç¨³å®š ~256
#             self.tokens_per_frame = 256 
#             self.target_size = (336, 336) 
#         elif '34B' in backbone_name:
#             self.tokens_per_frame = 576
#             self.target_size = None # LLaVA-Next å†…éƒ¨å¤„ç†
#         else:
#             self.tokens_per_frame = 256
#             self.target_size = None
            
#         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
#         # 1. Load CLIP Once (ç”¨äºåç»­ç‰¹å¾æå–å’Œå»ºå›¾ï¼Œä¸ç”¨äºæ£€æµ‹äº†)
#         self._load_clip_model()
        
#         # === ä¿®æ”¹ 2: åˆå§‹åŒ– TransNet V2 ===
#         if TransNetV2Detector is not None:
#             print("ğŸš€ [EventGraph] Initializing TransNet V2 Detector...")
#             self.shot_detector = TransNetV2Detector(device='cuda') # å¼ºåˆ¶ä½¿ç”¨cudaå¦‚æœå¯ç”¨
#         else:
#             self.shot_detector = None
#             print("âŒ [EventGraph] TransNet V2 Detector not available. Will use fallback.")

#     def _load_clip_model(self):
#         # Uses local path if available to save download time
#         local_path = "/root/hhq/models/clip-vit-large-patch14"
#         model_name = local_path if os.path.exists(local_path) else "openai/clip-vit-large-patch14"
        
#         try:
#             self.clip_processor = CLIPProcessor.from_pretrained(model_name)
#             self.clip_model = CLIPModel.from_pretrained(model_name).to(self.device)
#             self.clip_model.eval()
#         except Exception as e:
#             print(f"Warning: Loading CLIP from {model_name} failed ({e}), trying openai default.")
#             self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
#             self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
#             self.clip_model.eval()

#     def _detect_shot_boundaries(self, video_path):
#         """
#         ä½¿ç”¨ TransNet V2 è¿›è¡Œé•œå¤´åˆ†å‰²
#         """
#         # å¦‚æœæ£€æµ‹å™¨æ²¡åˆå§‹åŒ–ï¼Œç›´æ¥å›é€€
#         if self.shot_detector is None:
#             return self._fallback_windows(video_path)

#         try:
#             # === ä¿®æ”¹ 3: ç›´æ¥è°ƒç”¨ detect_shots ===
#             # TransNetV2Detector.detect_shots å·²ç»è¿”å›äº† [(start, end), ...] æ ¼å¼
#             # ä¸éœ€è¦å†æ‰‹åŠ¨è½¬æ¢ boundaries_to_events
#             events = self.shot_detector.detect_shots(video_path, threshold=0.5)
            
#             # è¿‡æ»¤æçŸ­çš„å™ªå£°ç‰‡æ®µ (< 0.5s)
#             events = [e for e in events if (e[1] - e[0]) >= 0.5]
            
#             # å¦‚æœæ£€æµ‹åˆ°çš„äº‹ä»¶å¤ªå°‘ï¼Œè¯´æ˜å¯èƒ½æ˜¯é•¿é•œå¤´æˆ–è€…æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥
#             if len(events) < 1: 
#                 print(f"  [EventGraph] Too few shots detected ({len(events)}), using fallback.")
#                 return self._fallback_windows(video_path)
            
#             return events

#         except Exception as e:
#             print(f"  âŒ [EventGraph] TransNet V2 failed ({e}), using fallback.")
#             return self._fallback_windows(video_path)

#     # _boundaries_to_events å‡½æ•°ç°åœ¨å¯ä»¥åˆ é™¤äº†ï¼Œå› ä¸º TransNet ç±»å†…éƒ¨å¤„ç†äº†
#     # ä½†ä¸ºäº†é˜²æ­¢æŸäº›å­ç±»ç»§æ‰¿è°ƒç”¨ï¼Œä½ å¯ä»¥ä¿ç•™å®ƒï¼Œæˆ–è€…ç›´æ¥åˆ é™¤ä»¥ä¿æŒä»£ç æ•´æ´ã€‚
#     # è¿™é‡Œæˆ‘æŠŠå®ƒç§»é™¤äº†ã€‚

#     def _extract_event_features(self, video_path, events):
#         # Batch processing for CLIP
#         if VideoReader is None: raise ImportError("decord required")
#         vr = VideoReader(video_path, ctx=cpu(0))
#         fps = vr.get_avg_fps()
        
#         representative_frames = []
#         valid_indices = []
        
#         for idx, (start_t, end_t) in enumerate(events):
#             mid_t = (start_t + end_t) / 2.0
#             # å®‰å…¨æ£€æŸ¥é˜²æ­¢è¶Šç•Œ
#             if mid_t * fps >= len(vr): 
#                 # å°è¯•å–æœ€åä¸€å¼ 
#                 frame_idx = len(vr) - 1
#             else:
#                 frame_idx = min(len(vr) - 1, int(mid_t * fps))
            
#             try:
#                 frame_np = vr[frame_idx].asnumpy()
#                 representative_frames.append(Image.fromarray(frame_np))
#                 valid_indices.append(idx)
#             except Exception as e:
#                 print(f"Error extracting frame at {mid_t}s: {e}")
#                 continue
        
#         if not representative_frames:
#             return torch.tensor([]), torch.tensor([]), []

#         # Batch Process
#         batch_size = 32
#         global_feats_list = []
#         local_feats_list = []
        
#         with torch.no_grad():
#             for i in range(0, len(representative_frames), batch_size):
#                 batch = representative_frames[i : i+batch_size]
#                 inputs = self.clip_processor(images=batch, return_tensors="pt", padding=True)
#                 inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
#                 # Global (CLS)
#                 g_feats = self.clip_model.get_image_features(**inputs)
#                 global_feats_list.append(g_feats)
                
#                 # Local (Patches)
#                 outputs = self.clip_model.vision_model(**inputs, output_hidden_states=True)
#                 l_feats = outputs.last_hidden_state[:, 1:, :] # Remove CLS
#                 local_feats_list.append(l_feats)
        
#         if len(global_feats_list) == 0:
#             return torch.tensor([]), torch.tensor([]), []

#         global_feats = torch.cat(global_feats_list, dim=0)
#         local_feats = torch.cat(local_feats_list, dim=0)
        
#         return global_feats, local_feats, representative_frames

#     def _construct_event_graph(self, global_feats, local_feats, events):
#         """
#         æ„å»ºè¯­ä¹‰-æ—¶åºå›¾
#         """
#         # ç¡®ä¿æ•°æ®åœ¨åŒä¸€è®¾å¤‡
#         global_feats = global_feats.to(self.device)
#         local_feats = local_feats.to(self.device)
        
#         N = global_feats.shape[0]
#         # 1. Compute Semantic Adjacency (Eq. 3, 4)
#         adj_semantic = compute_similarity_matrix(
#             global_feats, local_feats, 
#             tau=self.tau, 
#             event_times=events, 
#             threshold=self.delta
#         )
#         # 2. Compute Reachability (PageRank, Eq. 6)
#         Pi = compute_pagerank_matrix(adj_semantic, alpha=self.alpha)
#         return Pi

#     def _select_subgraph(self, Pi, question, global_feats, events):
#         """
#         CELF ç®—æ³•é€‰æ‹©å…³é”®å­å›¾
#         """
#         # 1. Encode Question
#         inputs = self.clip_processor(text=[question], return_tensors="pt", padding=True)
#         inputs = {k: v.to(self.device) for k, v in inputs.items()}
#         with torch.no_grad():
#             q_feat = self.clip_model.get_text_features(**inputs)
#             q_feat = q_feat / q_feat.norm(dim=-1, keepdim=True)
        
#         # 2. Calculate Query Relevance (Eq. 5)
#         # Normalize global feats
#         g_norm = global_feats / global_feats.norm(dim=-1, keepdim=True)
#         relevance = torch.mm(g_norm, q_feat.t()).squeeze() # (N,)
        
#         # Handle shape mismatch if only 1 event
#         if relevance.dim() == 0:
#             relevance = relevance.unsqueeze(0)
            
#         relevance = torch.clamp(relevance, min=0.0) # ReLU
        
#         # 3. Calculate Cost (Token consumption)
#         # ç®€å•çš„çº¿æ€§ä»£ä»·: æ¯ä¸ªäº‹ä»¶æ¶ˆè€— tokens_per_frame
#         costs = torch.full((len(events),), self.tokens_per_frame, device=self.device)
        
#         # 4. CELF Selection
#         selector = CELFSelector(Pi, relevance, costs, lambda_param=self.lambda_param)
#         selected_indices = selector.select(budget=self.token_budget)
        
#         return selected_indices

#     def _fallback_windows(self, video_path):
#         """
#         å¦‚æœ TransNet å¤±è´¥ï¼Œä½¿ç”¨ç­‰é—´éš”åˆ‡ç‰‡
#         """
#         print(f"âš ï¸ [EventGraph] Using fallback windows for {os.path.basename(video_path)}")
#         try:
#             cap = cv2.VideoCapture(video_path)
#             fps = cap.get(cv2.CAP_PROP_FPS)
#             count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#             cap.release()
#             duration = count / fps if fps > 0 else 0
#         except:
#             duration = 0
        
#         if duration == 0:
#             # å°è¯•ç”¨decordè·å–æ—¶é•¿
#             try:
#                 vr = VideoReader(video_path, ctx=cpu(0))
#                 duration = len(vr) / vr.get_avg_fps()
#             except:
#                 return [(0.0, 1.0)] # æœ€åçš„å…œåº•
        
#         events = []
#         # æ¯2ç§’åˆ‡ä¸€æ®µ (æ¯”ä¹‹å‰çš„é€»è¾‘ç¨å¾®å¯†é›†ä¸€ç‚¹ï¼Œä¿è¯è¦†ç›–)
#         step = 2.0
#         for t in np.arange(0, duration, step):
#             events.append((t, min(t + step, duration)))
        
#         # å¦‚æœè§†é¢‘æçŸ­
#         if not events:
#             events = [(0.0, min(1.0, duration))]
            
#         return events

#     # def _build_graph_cot_prompt(self, question, options, segments, adj_matrix, selected_indices):
#     #     event_timeline = [f"Event{i+1}" for i, _, _ in segments]
        
#     #     # æ ¼å¼åŒ–é€‰é¡¹å­—ç¬¦ä¸²
#     #     if isinstance(options, list):
#     #         # å¤„ç†å¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
#     #         options_clean = []
#     #         for opt in options:
#     #             if isinstance(opt, dict):
#     #                 options_clean.append(str(opt))
#     #             else:
#     #                 options_clean.append(str(opt))
            
#     #         # å¦‚æœæ˜¯A,B,C,Dæ ¼å¼
#     #         if len(options_clean) > 0 and (options_clean[0].startswith('A') or len(options_clean) <= 5):
#     #              options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options_clean)])
#     #         else:
#     #              options_str = "\n".join(options_clean)
#     #     else:
#     #         options_str = str(options)

#     #     prompt = (
#     #         f"Question: {question}\n"
#     #         f"Options:\n{options_str}\n"
#     #         f"Key Events Timeline: {' -> '.join(event_timeline)}\n"
#     #         f"Based on these visual events, reason step-by-step and choose the best answer."
#     #     )
#     #     return prompt
#     def _build_graph_cot_prompt(self, question, options, segments, adj_matrix, selected_indices):
#         # 1. Build a structured timeline with timestamps
#         timeline_str = ""
#         for idx, (start, end, original_idx) in enumerate(segments):
#             # Add explicit "Node" markers
#             timeline_str += f"- Node {idx+1} (Time: {start:.1f}s - {end:.1f}s): [Visual Content]\n"

#         # 2. Add "Graph Hints" (Optional: Tell the LLM which nodes are semantically related)
#         # We look at the adjacency matrix for selected nodes to find strong non-temporal links
#         graph_hints = []
#         for i in range(len(selected_indices)):
#             for j in range(len(selected_indices)):
#                 if i == j: continue
#                 # original graph indices
#                 u, v = selected_indices[i], selected_indices[j]
#                 # If there was a strong semantic edge in the original graph
#                 if adj_matrix[u, v] > 0.05: # Threshold for hint
#                     graph_hints.append(f"Node {i+1} is semantically related to Node {j+1}.")
        
#         hints_str = "\n".join(graph_hints[:5]) # Limit hints to avoid noise

#         # 3. Format Options
#         if isinstance(options, list):
#             options_clean = [str(o) for o in options]
#             options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options_clean)])
#         else:
#             options_str = str(options)

#         # 4. Structured CoT Prompt
#         prompt = (
#             f"You are analyzing a long video. I have selected key events for you based on a semantic graph.\n\n"
#             f"User Query: {question}\n\n"
#             f"Selected Key Events Timeline:\n{timeline_str}\n"
#             f"Key Semantic Connections identified by the graph:\n{hints_str}\n\n"
#             f"Options:\n{options_str}\n\n"
#             f"Instructions:\n"
#             f"1. Analyze the visual content of each Node relevant to the query.\n"
#             f"2. Connect the clues: If Node X and Node Y are related, combine their information.\n"
#             f"3. Reason step-by-step to answer the query.\n"
#             f"Answer:"
#         )
#         return prompt

#     def _build_simple_prompt(self, question, options):
#         """æ„å»ºç®€å•çš„ QA Promptï¼Œä¸éœ€è¦ Event Timeline"""
#         # æ ¼å¼åŒ–é€‰é¡¹
#         if isinstance(options, list) and options:
#             # æ¸…æ´—é€‰é¡¹ï¼Œç¡®ä¿éƒ½æ˜¯å­—ç¬¦ä¸²
#             clean_opts = []
#             for opt in options:
#                 clean_opts.append(str(opt))
                
#             options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(clean_opts)])
#             prompt = f"{question}\nOptions:\n{options_str}\nAnswer with the option letter directly."
#         else:
#             # å¼€æ”¾å¼é—®é¢˜
#             prompt = f"{question}\nAnswer the question in detail."
            
#         return prompt

#     def process_and_inference(self, video_path, question, options):
#         # 1. æ£€æµ‹äº‹ä»¶ (TransNet V2)
#         events = self._detect_shot_boundaries(video_path)
#         if not events: return "C"
        
#         # 2. æå–ç‰¹å¾
#         global_feats, local_feats, frames = self._extract_event_features(video_path, events)
#         if len(frames) == 0: return "C"

#         # 3. å»ºå›¾
#         Pi = self._construct_event_graph(global_feats, local_feats, events)
        
#         # 4. é€‰å›¾
#         sel_idx = self._select_subgraph(Pi, question, global_feats, events)
#         if not sel_idx: sel_idx = [0] # Fallback
        
#         # 5. å‡†å¤‡æ¨ç†æ•°æ®
#         valid_sel_idx = [i for i in sel_idx if i < len(frames)]
#         if not valid_sel_idx: valid_sel_idx = [0]
        
#         # --- IMPROVEMENT: Sort indices strictly by time ---
#         valid_sel_idx = sorted(valid_sel_idx)

#         # --- IMPROVEMENT: Force Resize to ensure Token Budget fits more frames ---
#         # For Qwen/LLaVA, 336x336 usually takes ~256 tokens. 
#         # This allows you to fit ~16 frames in a 4k budget, covering more timeline.
#         target_resolution = (336, 336) 
        
#         selected_frames = []
#         for i in valid_sel_idx:
#             img = frames[i]
#             # Resize guarantees token count matches your self.tokens_per_frame estimation
#             img_resized = img.resize(target_resolution, Image.BICUBIC)
#             selected_frames.append(img_resized)

#         selected_segments = [(events[i][0], events[i][1], i) for i in valid_sel_idx]
        
#         # 6. ç”Ÿæˆ Prompt (ç¨å¾®åŠ å¼ºä¸€ä¸‹ Promptï¼Œè®©å®ƒæ˜ç¡®è¾“å‡º)
#         # å»ºè®®åœ¨ prompt æœ€ååŠ ä¸€å¥æ˜ç¡®çš„æŒ‡ä»¤
#         prompt = self._build_graph_cot_prompt(
#             question, options, 
#             selected_segments, 
#             Pi, valid_sel_idx
#         )
#         prompt += "\nImportant: End your response with 'The answer is X.'"

#         # =======
#         # prompt = self._build_simple_prompt(question, options)

#         # 7. è°ƒç”¨ VLM æ¨ç†
#         # ğŸ”¥ ä¿®æ”¹è¿™é‡Œï¼šæ˜¾å¼ä¼ å…¥ max_new_tokens
#         # Video-MME çš„æ¨ç†é€šå¸¸éœ€è¦è¾ƒé•¿ç¯‡å¹…ï¼Œå»ºè®®è®¾ä¸º 1024 æˆ– 2048
#         return self.model.generate(
#             selected_frames, 
#             prompt, 
#             options, 
#             max_new_tokens=40960  # <--- å¢åŠ è¿™ä¸ªå‚æ•°
#         )



# # # import torch
# # # import numpy as np
# # # import cv2
# # # import os
# # # from PIL import Image
# # # from transformers import CLIPProcessor, CLIPModel
# # # from .base_method import BaseMethod
# # # from .graph_builder import compute_similarity_matrix, compute_pagerank_matrix
# # # from .celf_solver import CELFSelector

# # # try:
# # #     from .transnet_detector import TransNetV2Detector
# # # except ImportError:
# # #     # print("âš ï¸ Warning: Could not import TransNetV2Detector.")
# # #     TransNetV2Detector = None

# # # try:
# # #     from decord import VideoReader, cpu
# # # except ImportError:
# # #     print("âš ï¸ Warning: decord not installed")
# # #     VideoReader = None

# # # class EventGraphLMM(BaseMethod):
# # #     def __init__(self, args, model):
# # #         super().__init__(args, model)
        
# # #         # Params
# # #         self.tau = 30.0  
# # #         self.delta = 0.65 
# # #         self.alpha = 0.15 
# # #         self.lambda_param = 1.0 
# # #         self.token_budget = args.token_budget
        
# # #         # Detect token density
# # #         backbone_name = getattr(args, 'backbone', '')
# # #         if 'Qwen' in backbone_name:
# # #             self.tokens_per_frame = 256 
# # #             self.target_size = (336, 336) 
# # #         elif '34B' in backbone_name:
# # #             self.tokens_per_frame = 576
# # #             self.target_size = None 
# # #         else:
# # #             self.tokens_per_frame = 256
# # #             self.target_size = None
            
# # #         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
# # #         # Load CLIP
# # #         self._load_clip_model()
        
# # #         # Init TransNet
# # #         if TransNetV2Detector is not None:
# # #             # print("ğŸš€ [EventGraph] Initializing TransNet V2...")
# # #             self.shot_detector = TransNetV2Detector(device='cuda')
# # #         else:
# # #             self.shot_detector = None

# # #     def _load_clip_model(self):
# # #         local_path = "/root/hhq/models/clip-vit-large-patch14"
# # #         model_name = local_path if os.path.exists(local_path) else "openai/clip-vit-large-patch14"
# # #         try:
# # #             self.clip_processor = CLIPProcessor.from_pretrained(model_name)
# # #             self.clip_model = CLIPModel.from_pretrained(model_name).to(self.device)
# # #             self.clip_model.eval()
# # #         except Exception as e:
# # #             print(f"Warning: Loading CLIP failed, using default. {e}")
# # #             self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
# # #             self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
# # #             self.clip_model.eval()

# # #     def _detect_shot_boundaries(self, video_path):
# # #         if self.shot_detector is None:
# # #             return self._fallback_windows(video_path)

# # #         try:
# # #             events = self.shot_detector.detect_shots(video_path, threshold=0.5)
# # #             # Filter noise < 0.5s
# # #             events = [e for e in events if (e[1] - e[0]) >= 0.5]
            
# # #             # LVBench ä¼˜åŒ–: å¦‚æœæ£€æµ‹å‡ºçš„é•œå¤´è¿‡å¤š (>600)ï¼Œè¿›è¡Œåˆå¹¶æˆ–é™é‡‡æ ·
# # #             # é˜²æ­¢ Graph æ„å»ºè¿‡æ…¢
# # #             if len(events) > 600:
# # #                 # ç®€å•ç­–ç•¥ï¼šæ¯éš”ä¸€ä¸ªå–ä¸€ä¸ªï¼Œæˆ–è€…åˆå¹¶ç›¸é‚»
# # #                 events = events[::2] 

# # #             if len(events) < 1: 
# # #                 return self._fallback_windows(video_path)
# # #             return events

# # #         except Exception as e:
# # #             print(f"  âŒ TransNet error: {e}")
# # #             return self._fallback_windows(video_path)

# # #     def _extract_event_features(self, video_path, events):
# #         # if VideoReader is None: raise ImportError("decord required")
        
# #         # # æ˜¾å­˜ä¼˜åŒ–: å¼ºåˆ¶ä½¿ç”¨ CPU è¯»å–è§†é¢‘ï¼Œé˜²æ­¢ CUDA åˆå§‹åŒ–å†²çª
# #         # try:
# #         #     vr = VideoReader(video_path, ctx=cpu(0))
# #         # except Exception as e:
# #         #     print(f"âŒ Decord Init Failed: {e}")
# #         #     return torch.tensor([]), torch.tensor([]), []
            
# #         # fps = vr.get_avg_fps()
# #         # total_frames = len(vr)
        
# #         # representative_frames = []
# #         # valid_indices = []
        
# #         # # 1. è¯»å–å¸§
# #         # for idx, (start_t, end_t) in enumerate(events):
# #         #     mid_t = (start_t + end_t) / 2.0
# #         #     frame_idx = int(mid_t * fps)
# #         #     if frame_idx >= total_frames: frame_idx = total_frames - 1
            
# #         #     try:
# #         #         frame_np = vr[frame_idx].asnumpy()
# #         #         representative_frames.append(Image.fromarray(frame_np))
# #         #         valid_indices.append(idx)
# #         #     except:
# #         #         continue
        
# #         # # æ˜¾å¼é‡Šæ”¾ Decord èµ„æº
# #         # del vr
        
# #         # if not representative_frames:
# #         #     return torch.tensor([]), torch.tensor([]), []

# #         # # 2. æ‰¹é‡æå–ç‰¹å¾
# #         # batch_size = 16 
# #         # global_feats_list = []
# #         # local_feats_list = []
        
# #         # with torch.no_grad():
# #         #     for i in range(0, len(representative_frames), batch_size):
# #         #         batch = representative_frames[i : i+batch_size]
# #         #         inputs = self.clip_processor(images=batch, return_tensors="pt", padding=True)
# #         #         inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
# #         #         # --- Global Features ---
# #         #         # get_image_features é€šå¸¸è¿”å› Tensorï¼Œä½†é˜²å¾¡æ€§ç¼–ç¨‹é˜²æ­¢å®ƒè¿”å› ModelOutput
# #         #         g_feats = self.clip_model.get_image_features(**inputs)
                
# #         #         # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœè¿”å›çš„æ˜¯å¯¹è±¡è€Œä¸æ˜¯ Tensorï¼Œæå–å…¶ä¸­çš„ Tensor
# #         #         if not isinstance(g_feats, torch.Tensor):
# #         #             if hasattr(g_feats, 'image_embeds'): # æ–°ç‰ˆ Transformers å¯èƒ½çš„å­—æ®µ
# #         #                 g_feats = g_feats.image_embeds
# #         #             elif hasattr(g_feats, 'pooler_output'):
# #         #                 g_feats = g_feats.pooler_output
                
# #         #         # --- Local Features ---
# #         #         # vision_model è¿”å›çš„æ˜¯ BaseModelOutputWithPooling
# #         #         outputs = self.clip_model.vision_model(**inputs, output_hidden_states=True)
                
# #         #         if hasattr(outputs, 'last_hidden_state'):
# #         #             l_feats = outputs.last_hidden_state[:, 1:, :] # å»æ‰ CLS token
# #         #         elif isinstance(outputs, tuple):
# #         #             l_feats = outputs[0][:, 1:, :]
# #         #         else:
# #         #             raise ValueError(f"Unknown output type from vision_model: {type(outputs)}")
                
# #         #         # è½¬ CPU é‡Šæ”¾æ˜¾å­˜
# #         #         global_feats_list.append(g_feats.cpu())
# #         #         local_feats_list.append(l_feats.cpu())
        
# #         # if len(global_feats_list) == 0:
# #         #     return torch.tensor([]), torch.tensor([]), []

# #         # global_feats = torch.cat(global_feats_list, dim=0)
# #         # local_feats = torch.cat(local_feats_list, dim=0)
        
# #         # return global_feats, local_feats, representative_frames

# # #     def _construct_event_graph(self, global_feats, local_feats, events):
# # #         """
# # #         æ„å»ºå›¾ï¼Œæ”¯æŒ CPU Offload
# # #         """
# # #         N = global_feats.shape[0]
        
# # #         # å¦‚æœèŠ‚ç‚¹å¤ªå¤šï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU è®¡ç®—çŸ©é˜µï¼Œé¿å… OOM
# # #         # å¯¹äº LVBenchï¼ŒN å¯èƒ½è¾¾åˆ° 500+ï¼ŒN^2 çŸ©é˜µè¿˜è¡Œï¼Œä½†ä¸­é—´å˜é‡å¤§
# # #         compute_device = self.device
# # #         if N > 300: 
# # #             compute_device = torch.device('cpu')

# # #         global_feats = global_feats.to(compute_device)
# # #         local_feats = local_feats.to(compute_device)

# # #         # 1. Compute Semantic Adjacency
# # #         # ä¿®æ”¹ graph_builder é‡Œçš„å‡½æ•°è®©å®ƒæ¥å— device å‚æ•° (å¦‚æœæ”¯æŒ)
# # #         # æˆ–è€…ç¡®ä¿å®ƒæ˜¯çº¯ PyTorch æ“ä½œï¼Œä¼šè‡ªåŠ¨è·Ÿéš tensor çš„ device
# # #         try:
# # #             adj_semantic = compute_similarity_matrix(
# # #                 global_feats, local_feats, 
# # #                 tau=self.tau, 
# # #                 event_times=events, 
# # #                 threshold=self.delta
# # #             )
# # #         except RuntimeError:
# # #             # å¦‚æœ GPU çˆ†äº†ï¼Œå›é€€åˆ° CPU
# # #             print("âš ï¸ Graph construction OOM, switching to CPU.")
# # #             global_feats = global_feats.cpu()
# # #             local_feats = local_feats.cpu()
# # #             adj_semantic = compute_similarity_matrix(
# # #                 global_feats, local_feats, 
# # #                 tau=self.tau, 
# # #                 event_times=events, 
# # #                 threshold=self.delta
# # #             )
            
# # #         # 2. PageRank
# # #         Pi = compute_pagerank_matrix(adj_semantic, alpha=self.alpha)
        
# # #         # ç»“æœè½¬å› GPU (å¦‚æœ CELF éœ€è¦ GPU) æˆ–ä¿æŒ CPU
# # #         return Pi.to(self.device)

# # #     def _select_subgraph(self, Pi, question, global_feats, events):
# # #         # Encode Question
# # #         inputs = self.clip_processor(text=[question], return_tensors="pt", padding=True)
# # #         inputs = {k: v.to(self.device) for k, v in inputs.items()}
# # #         with torch.no_grad():
# # #             q_feat = self.clip_model.get_text_features(**inputs)
# # #             q_feat = q_feat / q_feat.norm(dim=-1, keepdim=True)
        
# # #         # Relevance calculation
# # #         # ç¡®ä¿ global_feats åœ¨ GPU ä¸Šè¿›è¡ŒçŸ©é˜µä¹˜æ³• (æ¯”è¾ƒå¿«)
# # #         # å¦‚æœæ˜¾å­˜æåº¦ç´§å¼ ï¼Œå¯ä»¥æŠŠ q_feat è½¬ CPU
# # #         g_feat_dev = global_feats.to(self.device)
# # #         g_norm = g_feat_dev / g_feat_dev.norm(dim=-1, keepdim=True)
        
# # #         relevance = torch.mm(g_norm, q_feat.t()).squeeze()
# # #         if relevance.dim() == 0: relevance = relevance.unsqueeze(0)
# # #         relevance = torch.clamp(relevance, min=0.0)
        
# # #         # Costs
# # #         costs = torch.full((len(events),), self.tokens_per_frame, device=self.device)
        
# # #         # CELF
# # #         # ç¡®ä¿ Pi ä¹Ÿåœ¨ device
# # #         Pi = Pi.to(self.device)
# # #         selector = CELFSelector(Pi, relevance, costs, lambda_param=self.lambda_param)
# # #         selected_indices = selector.select(budget=self.token_budget)
        
# # #         return selected_indices

# # #     def _fallback_windows(self, video_path):
# # #         try:
# # #             cap = cv2.VideoCapture(video_path)
# # #             fps = cap.get(cv2.CAP_PROP_FPS)
# # #             count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
# # #             cap.release()
# # #             duration = count / fps if fps > 0 else 0
# # #         except:
# # #             duration = 0
            
# # #         if duration == 0: return [(0.0, 1.0)]
        
# # #         events = []
# # #         # LVBench ä¼˜åŒ–: åŠ¨æ€æ­¥é•¿
# # #         # è§†é¢‘è¶Šé•¿ï¼Œåˆ‡ç‰‡è¶Šç¨€ç–ï¼Œé˜²æ­¢ç”Ÿæˆå‡ åƒä¸ªç‰‡æ®µ
# # #         if duration < 300: step = 2.0         # çŸ­è§†é¢‘: 2s
# # #         elif duration < 1800: step = 5.0      # 30åˆ†é’Ÿå†…: 5s
# # #         else: step = 10.0                     # é•¿è§†é¢‘: 10s
        
# # #         for t in np.arange(0, duration, step):
# # #             events.append((t, min(t + step, duration)))
            
# # #         if not events: events = [(0.0, min(1.0, duration))]
# # #         return events

# # #     def _build_simple_prompt(self, question, options):
# # #         if isinstance(options, list) and options:
# # #             clean_opts = [str(opt) for opt in options]
# # #             options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(clean_opts)])
# # #             prompt = f"{question}\nOptions:\n{options_str}\nAnswer with the option letter directly."
# # #         else:
# # #             prompt = f"{question}\nAnswer the question in detail."
# # #         return prompt

# # #     def process_and_inference(self, video_path, question, options):
# # #         # 1. æ£€æµ‹
# # #         events = self._detect_shot_boundaries(video_path)
# # #         if not events: return "C"
        
# # #         # 2. æå– (å·²åš CPU Offload)
# # #         global_feats, local_feats, frames = self._extract_event_features(video_path, events)
# # #         if len(frames) == 0: return "C"

# # #         # 3. å»ºå›¾
# # #         Pi = self._construct_event_graph(global_feats, local_feats, events)
        
# # #         # 4. é€‰å›¾
# # #         sel_idx = self._select_subgraph(Pi, question, global_feats, events)
# # #         if not sel_idx: sel_idx = [0]
        
# # #         # 5. å‡†å¤‡æ¨ç†æ•°æ®
# # #         valid_sel_idx = [i for i in sel_idx if i < len(frames)]
# # #         if not valid_sel_idx: valid_sel_idx = [0]
        
# # #         # æ’åº
# # #         valid_sel_idx = sorted(valid_sel_idx)

# # #         # ğŸ”¥ LVBench å…³é”®ä¼˜åŒ–: æ¨ç†å¸§æ•°ç¡¬æˆªæ–­ (Hard Cap)
# # #         # å³ä½¿ CELF é€‰äº† 100 å¼ ï¼Œæˆ‘ä»¬ä¹Ÿåªå– Top-K æˆ–è€…å‡åŒ€é‡‡æ ·åˆ° K
# # #         # é˜²æ­¢ Qwen çˆ†æ˜¾å­˜
# # #         MAX_INFERENCE_FRAMES = 64
# # #         if len(valid_sel_idx) > MAX_INFERENCE_FRAMES:
# # #             # ç®€å•çš„å‡åŒ€é™é‡‡æ ·
# # #             indices = np.linspace(0, len(valid_sel_idx) - 1, MAX_INFERENCE_FRAMES).astype(int)
# # #             valid_sel_idx = [valid_sel_idx[i] for i in indices]

# # #         selected_frames = []
# # #         for i in valid_sel_idx:
# # #             img = frames[i]
# # #             # Resize
# # #             if self.target_size:
# # #                 img_resized = img.resize(self.target_size, Image.BICUBIC)
# # #                 selected_frames.append(img_resized)
# # #             else:
# # #                 selected_frames.append(img)

# # #         # 6. Prompt
# # #         prompt = self._build_simple_prompt(question, options)

# # #         # 7. æ¨ç†
# # #         # max_new_tokens è®¾ä¸º 1024 (è¶³å¤Ÿå›ç­” A/B/C/D æˆ–ç®€çŸ­ CoT)
# # #         # ä¹‹å‰ 40960 ä¼šç›´æ¥çˆ†æ˜¾å­˜
# # #         return self.model.generate(
# # #             selected_frames, 
# # #             prompt, 
# # #             options, 
# # #             max_new_tokens=1024  
# # #         )

# # import torch
# # import numpy as np
# # import cv2
# # import os
# # from PIL import Image
# # from transformers import CLIPProcessor, CLIPModel
# # from .base_method import BaseMethod
# # from .graph_builder import compute_similarity_matrix, compute_pagerank_matrix
# # from .celf_solver import CELFSelector

# # try:
# #     from .transnet_detector import TransNetV2Detector
# # except ImportError:
# #     TransNetV2Detector = None

# # try:
# #     from decord import VideoReader, cpu, gpu
# # except ImportError:
# #     print("âš ï¸ Warning: decord not installed")
# #     VideoReader = None

# # class EventGraphLMM(BaseMethod):
# #     def __init__(self, args, model):
# #         super().__init__(args, model)
        
# #         # å‚æ•°
# #         self.tau = 30.0  
# #         self.delta = 0.65 
# #         self.alpha = 0.15 
# #         self.lambda_param = 1.0 
# #         self.token_budget = args.token_budget
        
# #         # Token ä¼°ç®—
# #         backbone_name = getattr(args, 'backbone', '')
# #         if 'Qwen' in backbone_name:
# #             self.tokens_per_frame = 256 
# #             self.target_size = (336, 336) 
# #         elif '34B' in backbone_name:
# #             self.tokens_per_frame = 576
# #             self.target_size = None 
# #         else:
# #             self.tokens_per_frame = 256
# #             self.target_size = None
            
# #         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
# #         # åŠ è½½ CLIP
# #         self._load_clip_model()
        
# #         # åˆå§‹åŒ– TransNet
# #         if TransNetV2Detector is not None:
# #             # ğŸ”¥ å¼ºåˆ¶ CPU æ¨¡å¼ï¼
# #             # 1. å½»åº•é¿å¼€ cuDNN ç‰ˆæœ¬å†²çªï¼ˆTensorFlow CPU ç‰ˆä¸éœ€è¦ cuDNNï¼‰
# #             # 2. 72B æ¨¡å‹éœ€è¦æ‰€æœ‰æ˜¾å­˜ï¼Œé•œå¤´æ£€æµ‹è¿™ç§å°ä»»åŠ¡äº¤ç»™ CPU ç»°ç»°æœ‰ä½™
# #             print("ğŸš€ [EventGraph] Initializing TransNet V2 on CPU (Safe Mode)...")
# #             try:
# #                 self.shot_detector = TransNetV2Detector(device='cuda')
# #             except Exception as e:
# #                 print(f"âš ï¸ TransNet Init Failed: {e}. Will use fallback windows.")
# #                 self.shot_detector = None
# #         else:
# #             self.shot_detector = None

# #     def _load_clip_model(self):
# #         local_path = "/root/hhq/models/clip-vit-large-patch14"
# #         model_name = local_path if os.path.exists(local_path) else "openai/clip-vit-large-patch14"
# #         try:
# #             self.clip_processor = CLIPProcessor.from_pretrained(model_name)
# #             self.clip_model = CLIPModel.from_pretrained(model_name).to(self.device)
# #             self.clip_model.eval()
# #         except Exception as e:
# #             print(f"Warning: Loading CLIP failed ({e}), using default.")
# #             self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
# #             self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
# #             self.clip_model.eval()

# #     def _detect_shot_boundaries(self, video_path):
# #         if self.shot_detector is None:
# #             return self._fallback_windows(video_path)
# #         try:
# #             # è¿™é‡Œçš„ threshold å¯ä»¥ç¨å¾®è°ƒä½ä¸€ç‚¹ä»¥è·å¾—æ›´å¤šç»†ç²’åº¦äº‹ä»¶
# #             events = self.shot_detector.detect_shots(video_path, threshold=0.3)
# #             events = [e for e in events if (e[1] - e[0]) >= 0.5]
            
# #             # LVBench é•¿è§†é¢‘ä¼˜åŒ–ï¼šé™åˆ¶æœ€å¤§äº‹ä»¶æ•°
# #             if len(events) > 1000:
# #                 events = events[::2] 

# #             if len(events) < 1: 
# #                 return self._fallback_windows(video_path)
# #             return events
# #         except Exception as e:
# #             print(f"  âŒ TransNet error: {e}")
# #             return self._fallback_windows(video_path)

# #     def _extract_event_features(self, video_path, events):
# #         if VideoReader is None: raise ImportError("decord required")
        
# #         try:
# #             vr = VideoReader(video_path, ctx=cpu(0))
# #         except:
# #             return torch.tensor([]), torch.tensor([]), []
            
# #         fps = vr.get_avg_fps()
# #         total_frames = len(vr)
        
# #         representative_frames = []
        
# #         # 1. è¯»å–å¸§ (CPU -> RAM)
# #         for idx, (start_t, end_t) in enumerate(events):
# #             mid_t = (start_t + end_t) / 2.0
# #             frame_idx = int(mid_t * fps)
# #             if frame_idx >= total_frames: frame_idx = total_frames - 1
            
# #             try:
# #                 frame_np = vr[frame_idx].asnumpy()
# #                 representative_frames.append(Image.fromarray(frame_np))
# #             except:
# #                 continue
        
# #         del vr 
# #         if not representative_frames:
# #             return torch.tensor([]), torch.tensor([]), []

# #         # 2. æ‰¹é‡æå–ç‰¹å¾
# #         batch_size = 64
# #         global_feats_list = []
# #         local_feats_list = []
        
# #         with torch.no_grad():
# #             for i in range(0, len(representative_frames), batch_size):
# #                 batch = representative_frames[i : i+batch_size]
# #                 inputs = self.clip_processor(images=batch, return_tensors="pt", padding=True)
# #                 inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
# #                 # Global
# #                 g_out = self.clip_model.get_image_features(**inputs)
# #                 # ç±»å‹æ£€æŸ¥
# #                 if isinstance(g_out, torch.Tensor):
# #                     g_feats = g_out
# #                 elif hasattr(g_out, 'image_embeds'):
# #                     g_feats = g_out.image_embeds
# #                 elif hasattr(g_out, 'pooler_output'):
# #                     g_feats = g_out.pooler_output
# #                 else:
# #                     g_feats = g_out[0]
                
# #                 # Local
# #                 l_out = self.clip_model.vision_model(**inputs, output_hidden_states=True)
# #                 # ç±»å‹æ£€æŸ¥
# #                 if hasattr(l_out, 'last_hidden_state'):
# #                     l_feats = l_out.last_hidden_state[:, 1:, :] 
# #                 elif isinstance(l_out, tuple):
# #                     l_feats = l_out[0][:, 1:, :]
# #                 else:
# #                     l_feats = g_feats.unsqueeze(1)

# #                 global_feats_list.append(g_feats) 
# #                 local_feats_list.append(l_feats)
        
# #         if len(global_feats_list) == 0:
# #             return torch.tensor([]), torch.tensor([]), []

# #         global_feats = torch.cat(global_feats_list, dim=0)
# #         local_feats = torch.cat(local_feats_list, dim=0)
        
# #         return global_feats, local_feats, representative_frames

# #     def _construct_event_graph(self, global_feats, local_feats, events):
# #         # æ­¤æ—¶æ•°æ®å·²ç»åœ¨ GPU ä¸Šäº†ï¼Œç›´æ¥ç®—
# #         adj_semantic = compute_similarity_matrix(
# #             global_feats, local_feats, 
# #             tau=self.tau, 
# #             event_times=events, 
# #             threshold=self.delta
# #         )
# #         Pi = compute_pagerank_matrix(adj_semantic, alpha=self.alpha)
# #         return Pi

# #     def _select_subgraph(self, Pi, question, global_feats, events):
# #         # 1. Encode Question
# #         inputs = self.clip_processor(text=[question], return_tensors="pt", padding=True)
# #         inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
# #         with torch.no_grad():
# #             q_out = self.clip_model.get_text_features(**inputs)
            
# #             # ğŸ”¥ [å…³é”®ä¿®å¤]ï¼šé’ˆå¯¹ Text Features çš„ç±»å‹æ£€æŸ¥
# #             if isinstance(q_out, torch.Tensor):
# #                 q_feat = q_out
# #             elif hasattr(q_out, 'text_embeds'):
# #                 q_feat = q_out.text_embeds
# #             elif hasattr(q_out, 'pooler_output'):
# #                 q_feat = q_out.pooler_output
# #             else:
# #                 q_feat = q_out[0] # Tuple fallback
            
# #             # ç°åœ¨ q_feat è‚¯å®šæ˜¯ Tensorï¼Œå¯ä»¥å®‰å…¨è°ƒç”¨ norm
# #             q_feat = q_feat / q_feat.norm(dim=-1, keepdim=True)
        
# #         # 2. Relevance calculation (GPU)
# #         g_norm = global_feats / global_feats.norm(dim=-1, keepdim=True)
# #         relevance = torch.mm(g_norm, q_feat.t()).squeeze()
# #         if relevance.dim() == 0: relevance = relevance.unsqueeze(0)
# #         relevance = torch.clamp(relevance, min=0.0)
        
# #         costs = torch.full((len(events),), self.tokens_per_frame, device=self.device)
        
# #         # 3. CELF Selection
# #         selector = CELFSelector(Pi, relevance, costs, lambda_param=self.lambda_param)
# #         selected_indices = selector.select(budget=self.token_budget)
        
# #         return selected_indices

# #     def _fallback_windows(self, video_path):
# #         try:
# #             cap = cv2.VideoCapture(video_path)
# #             fps = cap.get(cv2.CAP_PROP_FPS)
# #             count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
# #             cap.release()
# #             duration = count / fps if fps > 0 else 0
# #         except:
# #             duration = 0
            
# #         if duration == 0: return [(0.0, 1.0)]
        
# #         events = []
# #         if duration < 300: step = 2.0
# #         elif duration < 1800: step = 5.0
# #         else: step = 10.0
        
# #         for t in np.arange(0, duration, step):
# #             events.append((t, min(t + step, duration)))
            
# #         if not events: events = [(0.0, min(1.0, duration))]
# #         return events

# #     def _build_simple_prompt(self, question, options):
# #         if isinstance(options, list) and options:
# #             clean_opts = [str(opt) for opt in options]
# #             options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(clean_opts)])
# #             prompt = f"{question}\nOptions:\n{options_str}\nAnswer with the option letter directly."
# #         else:
# #             prompt = f"{question}\nAnswer the question in detail."
# #         return prompt

# #     def process_and_inference(self, video_path, question, options):
# #         # 1. æ£€æµ‹
# #         events = self._detect_shot_boundaries(video_path)
# #         if not events: return "C"
        
# #         # 2. æå–
# #         global_feats, local_feats, frames = self._extract_event_features(video_path, events)
# #         if len(frames) == 0: return "C"

# #         # 3. å»ºå›¾
# #         Pi = self._construct_event_graph(global_feats, local_feats, events)
        
# #         # 4. é€‰å›¾
# #         sel_idx = self._select_subgraph(Pi, question, global_feats, events)
# #         if not sel_idx: sel_idx = [0]
        
# #         valid_sel_idx = [i for i in sel_idx if i < len(frames)]
# #         if not valid_sel_idx: valid_sel_idx = [0]
# #         valid_sel_idx = sorted(valid_sel_idx)

# #         # æ¨ç†å¸§æ•°ä¼˜åŒ– (128å¸§)
# #         MAX_INFERENCE_FRAMES = 128
# #         if len(valid_sel_idx) > MAX_INFERENCE_FRAMES:
# #             indices = np.linspace(0, len(valid_sel_idx) - 1, MAX_INFERENCE_FRAMES).astype(int)
# #             valid_sel_idx = [valid_sel_idx[i] for i in indices]

# #         selected_frames = []
# #         for i in valid_sel_idx:
# #             img = frames[i]
# #             if self.target_size:
# #                 img_resized = img.resize(self.target_size, Image.BICUBIC)
# #                 selected_frames.append(img_resized)
# #             else:
# #                 selected_frames.append(img)

# #         # 6. Prompt
# #         prompt = self._build_simple_prompt(question, options)

# #         # 7. æ¨ç† (Token 2048)
# #         return self.model.generate(
# #             selected_frames, 
# #             prompt, 
# #             options, 
# #             max_new_tokens=10240
# #         )

import torch
import numpy as np
import cv2
import os
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from .base_method import BaseMethod
from .graph_builder import compute_similarity_matrix, compute_pagerank_matrix
from .celf_solver import CELFSelector

# âœ… æ¢å¤ TransNet å¯¼å…¥
try:
    from .transnet_detector import TransNetV2Detector
except ImportError:
    print("âš ï¸ Warning: Could not import TransNetV2Detector.")
    TransNetV2Detector = None

try:
    from decord import VideoReader, cpu
except ImportError:
    print("âš ï¸ Warning: decord not installed")
    VideoReader = None

class EventGraphLMM(BaseMethod):
    def __init__(self, args, model):
        super().__init__(args, model)
        
        # Params
        self.tau = 30.0  
        self.delta = 0.65 
        self.alpha = 0.15 
        self.lambda_param = 1.0 
        self.token_budget = args.token_budget
        
        # Token ä¼°ç®—
        backbone_name = getattr(args, 'backbone', '')
        if 'Qwen' in backbone_name:
            self.tokens_per_frame = 256 
            self.target_size = (336, 336) 
        elif '34B' in backbone_name:
            self.tokens_per_frame = 576
            self.target_size = None 
        else:
            self.tokens_per_frame = 256
            self.target_size = None
            
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # 1. Load CLIP
        self._load_clip_model()
        
        # âœ… 2. åˆå§‹åŒ– TransNet (ä½ å·²è§£å†³ CUDA å†²çªï¼Œè¿™é‡Œæ¢å¤ä½¿ç”¨ GPU)
        if TransNetV2Detector is not None:
            print("ğŸš€ [EventGraph] Initializing TransNet V2 Detector on GPU...")
            try:
                self.shot_detector = TransNetV2Detector(device='cuda') 
            except Exception as e:
                print(f"âŒ TransNet Init Failed: {e}. Using fallback.")
                self.shot_detector = None
        else:
            self.shot_detector = None

    def _load_clip_model(self):
        local_path = "/root/hhq/models/clip-vit-large-patch14"
        model_name = local_path if os.path.exists(local_path) else "openai/clip-vit-large-patch14"
        try:
            self.clip_processor = CLIPProcessor.from_pretrained(model_name)
            self.clip_model = CLIPModel.from_pretrained(model_name).to(self.device)
            self.clip_model.eval()
        except Exception as e:
            print(f"Warning: Loading CLIP failed ({e}), using default.")
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
            self.clip_model.eval()

    def _detect_shot_boundaries(self, video_path):
        """
        ä½¿ç”¨ TransNet V2 è¿›è¡Œé•œå¤´åˆ†å‰²
        """
        if self.shot_detector is None:
            return self._fallback_windows(video_path)

        try:
            # TransNet æ£€æµ‹
            events = self.shot_detector.detect_shots(video_path, threshold=0.5)
            
            # è¿‡æ»¤çŸ­ç‰‡æ®µ
            events = [e for e in events if (e[1] - e[0]) >= 0.5]
            
            # ä¼˜åŒ–ï¼šå¦‚æœé•œå¤´å¤ªå¤šï¼Œé™é‡‡æ ·é˜²æ­¢ Graph è¿‡å¤§
            if len(events) > 800:
                events = events[::2] 

            if len(events) < 1: 
                return self._fallback_windows(video_path)
            return events

        except Exception as e:
            print(f"  âŒ TransNet execution failed ({e}), using fallback.")
            return self._fallback_windows(video_path)

    def _extract_event_features(self, video_path, events):
        # æ—¢ç„¶æ²¡æœ‰ CUDA å†²çªï¼Œæˆ‘ä»¬å°è¯•ç”¨ CPU æ¨¡å¼çš„ Decord è¯»å–
        # (TransNet ç”¨ GPUï¼Œè¿™é‡Œè¯»å–ç”¨ CPUï¼Œäº’ä¸å¹²æ‰°)
        if VideoReader is None: raise ImportError("decord required")
        
        try:
            vr = VideoReader(video_path, ctx=cpu(0))
        except:
            return torch.tensor([]), torch.tensor([]), []
            
        fps = vr.get_avg_fps()
        total_frames = len(vr)
        
        representative_frames = []
        
        for idx, (start_t, end_t) in enumerate(events):
            mid_t = (start_t + end_t) / 2.0
            frame_idx = int(mid_t * fps)
            if frame_idx >= total_frames: frame_idx = total_frames - 1
            
            try:
                frame_np = vr[frame_idx].asnumpy()
                representative_frames.append(Image.fromarray(frame_np))
            except:
                continue
        
        del vr # é‡Šæ”¾èµ„æº
        
        if not representative_frames:
            return torch.tensor([]), torch.tensor([]), []

        # Batch Process
        batch_size = 64
        global_feats_list = []
        local_feats_list = []
        
        with torch.no_grad():
            for i in range(0, len(representative_frames), batch_size):
                batch = representative_frames[i : i+batch_size]
                inputs = self.clip_processor(images=batch, return_tensors="pt", padding=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # --- Global Features ---
                g_out = self.clip_model.get_image_features(**inputs)
                
                # ğŸ”¥ [ä¿®å¤æŠ¥é”™]ï¼šå¦‚æœæ˜¯å¯¹è±¡ï¼Œæå– Tensor
                if isinstance(g_out, torch.Tensor):
                    g_feats = g_out
                elif hasattr(g_out, 'image_embeds'):
                    g_feats = g_out.image_embeds
                elif hasattr(g_out, 'pooler_output'):
                    g_feats = g_out.pooler_output
                else:
                    g_feats = g_out[0]
                
                # --- Local Features ---
                l_out = self.clip_model.vision_model(**inputs, output_hidden_states=True)
                
                # ğŸ”¥ [ä¿®å¤æŠ¥é”™]ï¼šæå– Tensor
                if hasattr(l_out, 'last_hidden_state'):
                    l_feats = l_out.last_hidden_state[:, 1:, :] 
                elif isinstance(l_out, tuple):
                    l_feats = l_out[0][:, 1:, :]
                else:
                    l_feats = g_feats.unsqueeze(1) # å…œåº•

                global_feats_list.append(g_feats)
                local_feats_list.append(l_feats)
        
        if len(global_feats_list) == 0:
            return torch.tensor([]), torch.tensor([]), []

        global_feats = torch.cat(global_feats_list, dim=0)
        local_feats = torch.cat(local_feats_list, dim=0)
        
        return global_feats, local_feats, representative_frames

    def _construct_event_graph(self, global_feats, local_feats, events):
        # èŠ‚ç‚¹å¤šæ—¶ä½¿ç”¨ CPU ç®—å›¾
        compute_device = self.device
        if global_feats.shape[0] > 600: 
            compute_device = torch.device('cpu')

        global_feats = global_feats.to(compute_device)
        local_feats = local_feats.to(compute_device)

        try:
            adj_semantic = compute_similarity_matrix(
                global_feats, local_feats, 
                tau=self.tau, 
                event_times=events, 
                threshold=self.delta
            )
        except RuntimeError:
            print("âš ï¸ Graph OOM, switching to CPU.")
            global_feats = global_feats.cpu()
            local_feats = local_feats.cpu()
            adj_semantic = compute_similarity_matrix(global_feats, local_feats, tau=self.tau, event_times=events, threshold=self.delta)
            
        Pi = compute_pagerank_matrix(adj_semantic, alpha=self.alpha)
        return Pi.to(self.device)

    def _select_subgraph(self, Pi, question, global_feats, events):
        inputs = self.clip_processor(text=[question], return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            q_out = self.clip_model.get_text_features(**inputs)
            
            # ğŸ”¥ [ä¿®å¤æŠ¥é”™]ï¼šæ–‡æœ¬ç‰¹å¾ä¹ŸåšåŒæ ·æ£€æŸ¥
            if isinstance(q_out, torch.Tensor):
                q_feat = q_out
            elif hasattr(q_out, 'text_embeds'):
                q_feat = q_out.text_embeds
            elif hasattr(q_out, 'pooler_output'):
                q_feat = q_out.pooler_output
            else:
                q_feat = q_out[0]
            
            q_feat = q_feat / q_feat.norm(dim=-1, keepdim=True)
        
        g_norm = global_feats / global_feats.norm(dim=-1, keepdim=True)
        relevance = torch.mm(g_norm, q_feat.t()).squeeze()
        if relevance.dim() == 0: relevance = relevance.unsqueeze(0)
        relevance = torch.clamp(relevance, min=0.0)
        
        costs = torch.full((len(events),), self.tokens_per_frame, device=self.device)
        
        selector = CELFSelector(Pi, relevance, costs, lambda_param=self.lambda_param)
        selected_indices = selector.select(budget=self.token_budget)
        
        return selected_indices

    def _fallback_windows(self, video_path):
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()
            duration = count / fps if fps > 0 else 0
        except:
            duration = 0
            
        if duration == 0: return [(0.0, 1.0)]
        
        events = []
        step = 5.0
        if duration > 1800: step = 10.0
        
        for t in np.arange(0, duration, step):
            events.append((t, min(t + step, duration)))
            
        if not events: events = [(0.0, min(1.0, duration))]
        return events

    def _build_graph_cot_prompt(self, question, options, segments, adj_matrix, selected_indices):
        timeline_str = ""
        for idx, (start, end, original_idx) in enumerate(segments):
            timeline_str += f"- Node {idx+1} (Time: {start:.1f}s - {end:.1f}s): [Visual Content]\n"

        graph_hints = []
        for i in range(len(selected_indices)):
            for j in range(len(selected_indices)):
                if i == j: continue
                u, v = selected_indices[i], selected_indices[j]
                if adj_matrix[u, v] > 0.05: 
                    graph_hints.append(f"Node {i+1} is semantically related to Node {j+1}.")
        hints_str = "\n".join(graph_hints[:5])

        if isinstance(options, list):
            clean_opts = [str(o) for o in options]
            options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(clean_opts)])
        else:
            options_str = str(options)

        prompt = (
            f"You are analyzing a long video. I have selected key events for you based on a semantic graph.\n\n"
            f"User Query: {question}\n\n"
            f"Selected Key Events Timeline:\n{timeline_str}\n"
            f"Key Semantic Connections identified by the graph:\n{hints_str}\n\n"
            f"Options:\n{options_str}\n\n"
            f"Instructions:\n"
            f"1. Analyze the visual content of each Node relevant to the query.\n"
            f"2. Connect the clues: If Node X and Node Y are related, combine their information.\n"
            f"3. Reason step-by-step to answer the query.\n"
            f"Answer:"
        )
        return prompt

    def process_and_inference(self, video_path, question, options):
        # 1. TransNet æ£€æµ‹ (GPU)
        events = self._detect_shot_boundaries(video_path)
        if not events: return "C"
        
        # 2. æå–ç‰¹å¾ (å«ç±»å‹ä¿®å¤)
        global_feats, local_feats, frames = self._extract_event_features(video_path, events)
        if len(frames) == 0: return "C"

        # 3. å»ºå›¾
        Pi = self._construct_event_graph(global_feats, local_feats, events)
        
        # 4. é€‰å›¾
        sel_idx = self._select_subgraph(Pi, question, global_feats, events)
        if not sel_idx: sel_idx = [0]
        
        valid_sel_idx = [i for i in sel_idx if i < len(frames)]
        if not valid_sel_idx: valid_sel_idx = [0]
        valid_sel_idx = sorted(valid_sel_idx)

        # é™åˆ¶å¸§æ•°ï¼Œé˜²æ­¢ Context æº¢å‡º
        MAX_INFERENCE_FRAMES = 96
        if len(valid_sel_idx) > MAX_INFERENCE_FRAMES:
            indices = np.linspace(0, len(valid_sel_idx) - 1, MAX_INFERENCE_FRAMES).astype(int)
            valid_sel_idx = [valid_sel_idx[i] for i in indices]

        selected_frames = []
        for i in valid_sel_idx:
            img = frames[i]
            if self.target_size:
                img_resized = img.resize(self.target_size, Image.BICUBIC)
                selected_frames.append(img_resized)
            else:
                selected_frames.append(img)

        prompt = self._build_graph_cot_prompt(
            question, options, 
            [(events[i][0], events[i][1], i) for i in valid_sel_idx], 
            Pi, valid_sel_idx
        )
        prompt += "\nImportant: End your response with 'The answer is X.'"

        # ğŸ”¥ å…³é”®ï¼šæ”¹å› 2048ã€‚40960 ç»å¯¹ä¼šçˆ†æ˜¾å­˜ã€‚
        return self.model.generate(
            selected_frames, 
            prompt, 
            options, 
            max_new_tokens=20480 
        )