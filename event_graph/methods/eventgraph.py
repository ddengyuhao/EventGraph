# # /root/icml2026/event_graph/code/eventgraph.py
# import torch
# import numpy as np
# import cv2  # <--- ä¿®å¤æŠ¥é”™: ä¹‹å‰ç¼ºå°‘è¿™ä¸ªå¯¼å…¥
# import os
# from PIL import Image
# from transformers import CLIPProcessor, CLIPModel
# from .base_method import BaseMethod
# from .graph_builder import compute_similarity_matrix, compute_pagerank_matrix
# from .celf_solver import CELFSelector
# from .uboco_detector import UbocoDetector

# try:
#     from decord import VideoReader, cpu
# except ImportError:
#     print("âš ï¸ Warning: decord not installed")
#     VideoReader = None

# class EventGraphLMM(BaseMethod):
#     def __init__(self, args, model):
#         super().__init__(args, model)
        
#         # Params from Paper Section 4.1
#         self.tau = 30.0  
#         self.delta = 0.65 
#         self.alpha = 0.15 
#         self.lambda_param = 1.0 
#         self.token_budget = args.token_budget
        
#         # Detect token density for budget calculation
#         backbone_name = getattr(args, 'backbone', '')
#         if '34B' in backbone_name:
#             self.tokens_per_frame = 576 
#         elif 'Qwen' in backbone_name:
#             self.tokens_per_frame = 256 
#         else:
#             self.tokens_per_frame = 256 # Video-LLaVA-7B default
            
#         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
#         # 1. Load CLIP Once
#         self._load_clip_model()
        
#         # 2. Initialize Uboco with the SHARED model
#         self.shot_detector = UbocoDetector(
#             device=self.device, 
#             clip_model=self.clip_model, 
#             clip_processor=self.clip_processor
#         )

#     def _load_clip_model(self):
#         # Uses local path if available to save download time
#         local_path = "/root/hhq/models/clip-vit-large-patch14"
#         model_name = local_path if os.path.exists(local_path) else "openai/clip-vit-large-patch14"
        
#         try:
#             self.clip_processor = CLIPProcessor.from_pretrained(model_name)
#             self.clip_model = CLIPModel.from_pretrained(model_name).to(self.device)
#             self.clip_model.eval()
#         except Exception as e:
#             print(f"Warning: Loading CLIP from {model_name} failed ({e}), trying openai default.")
#             self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
#             self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
#             self.clip_model.eval()

#     def _detect_shot_boundaries(self, video_path):
#         # Delegate to the shared-model Uboco detector
#         try:
#             # Uboco return timestamps (e.g., [2.5, 5.1, ...])
#             boundaries = self.shot_detector.detect(video_path, sample_rate=2) 
#             # Convert boundaries to event intervals (start, end)
#             events = self._boundaries_to_events(boundaries, video_path)
            
#             # Filter noise < 1s
#             events = [e for e in events if (e[1] - e[0]) >= 1.0]
            
#             # If too few events, use fallback
#             if len(events) < 3: 
#                 return self._fallback_windows(video_path)
#             return events
#         except Exception as e:
#             print(f"  [EventGraph] Uboco failed ({e}), using fallback.")
#             return self._fallback_windows(video_path)

#     def _boundaries_to_events(self, boundaries, video_path):
#         """
#         ä¿®å¤æŠ¥é”™: å°†æ—¶é—´æˆ³åˆ—è¡¨è½¬æ¢ä¸º (start, end) å…ƒç»„åˆ—è¡¨
#         """
#         # è·å–è§†é¢‘æ€»æ—¶é•¿
#         cap = cv2.VideoCapture(video_path)
#         fps = cap.get(cv2.CAP_PROP_FPS)
#         frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         duration = frame_count / fps if fps > 0 else 0
#         cap.release()

#         if duration == 0: return []

#         # æ„é€ åŒºé—´
#         sorted_bounds = sorted([0.0] + list(boundaries) + [duration])
#         # å»é‡
#         sorted_bounds = sorted(list(set(sorted_bounds)))
        
#         events = []
#         for i in range(len(sorted_bounds) - 1):
#             start = sorted_bounds[i]
#             end = sorted_bounds[i+1]
#             events.append((start, end))
#         return events

#     def _extract_event_features(self, video_path, events):
#         # Batch processing for CLIP
#         if VideoReader is None: raise ImportError("decord required")
#         vr = VideoReader(video_path, ctx=cpu(0))
#         fps = vr.get_avg_fps()
        
#         representative_frames = []
#         valid_indices = []
        
#         for idx, (start_t, end_t) in enumerate(events):
#             mid_t = (start_t + end_t) / 2.0
#             # å®‰å…¨æ£€æŸ¥é˜²æ­¢è¶Šç•Œ
#             if mid_t * fps >= len(vr): continue
            
#             frame_idx = min(len(vr) - 1, int(mid_t * fps))
#             frame_np = vr[frame_idx].asnumpy()
#             representative_frames.append(Image.fromarray(frame_np))
#             valid_indices.append(idx)
        
#         # Batch Process
#         batch_size = 32
#         global_feats_list = []
#         local_feats_list = []
        
#         with torch.no_grad():
#             for i in range(0, len(representative_frames), batch_size):
#                 batch = representative_frames[i : i+batch_size]
#                 inputs = self.clip_processor(images=batch, return_tensors="pt", padding=True)
#                 inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
#                 # Global (CLS)
#                 g_feats = self.clip_model.get_image_features(**inputs)
#                 global_feats_list.append(g_feats)
                
#                 # Local (Patches)
#                 outputs = self.clip_model.vision_model(**inputs, output_hidden_states=True)
#                 l_feats = outputs.last_hidden_state[:, 1:, :] # Remove CLS
#                 local_feats_list.append(l_feats)
        
#         if len(global_feats_list) == 0:
#             return torch.tensor([]), torch.tensor([]), []

#         global_feats = torch.cat(global_feats_list, dim=0)
#         local_feats = torch.cat(local_feats_list, dim=0)
        
#         return global_feats, local_feats, representative_frames

#     def _construct_event_graph(self, global_feats, local_feats, events):
#         """
#         æ„å»ºè¯­ä¹‰-æ—¶åºå›¾
#         """
#         N = global_feats.shape[0]
#         # 1. Compute Semantic Adjacency (Eq. 3, 4)
#         adj_semantic = compute_similarity_matrix(
#             global_feats, local_feats, 
#             tau=self.tau, 
#             event_times=events, 
#             threshold=self.delta
#         )
#         # 2. Compute Reachability (PageRank, Eq. 6)
#         Pi = compute_pagerank_matrix(adj_semantic, alpha=self.alpha)
#         return Pi

#     def _select_subgraph(self, Pi, question, global_feats, events):
#         """
#         CELF ç®—æ³•é€‰æ‹©å…³é”®å­å›¾
#         """
#         # 1. Encode Question
#         inputs = self.clip_processor(text=[question], return_tensors="pt", padding=True)
#         inputs = {k: v.to(self.device) for k, v in inputs.items()}
#         with torch.no_grad():
#             q_feat = self.clip_model.get_text_features(**inputs)
#             q_feat = q_feat / q_feat.norm(dim=-1, keepdim=True)
        
#         # 2. Calculate Query Relevance (Eq. 5)
#         g_norm = global_feats / global_feats.norm(dim=-1, keepdim=True)
#         relevance = torch.mm(g_norm, q_feat.t()).squeeze() # (N,)
#         relevance = torch.clamp(relevance, min=0.0) # ReLU
        
#         # 3. Calculate Cost (Token consumption)
#         # ç®€å•çš„çº¿æ€§ä»£ä»·: æ¯ä¸ªäº‹ä»¶æ¶ˆè€— tokens_per_frame
#         costs = torch.full((len(events),), self.tokens_per_frame, device=self.device)
        
#         # 4. CELF Selection
#         selector = CELFSelector(Pi, relevance, costs, lambda_param=self.lambda_param)
#         selected_indices = selector.select(budget=self.token_budget)
        
#         return selected_indices

#     def _fallback_windows(self, video_path):
#         """
#         å¦‚æœ Uboco å¤±è´¥ï¼Œä½¿ç”¨ç­‰é—´éš”åˆ‡ç‰‡
#         """
#         cap = cv2.VideoCapture(video_path)
#         fps = cap.get(cv2.CAP_PROP_FPS)
#         count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
#         cap.release()
        
#         duration = count / fps if fps > 0 else 0
#         events = []
#         # æ¯2ç§’åˆ‡ä¸€æ®µ
#         step = 2.0
#         for t in np.arange(0, duration, step):
#             events.append((t, min(t + step, duration)))
        
#         # å¦‚æœè§†é¢‘æçŸ­æˆ–è€…è¯»ä¸åˆ°ï¼Œç»™ä¸€ä¸ªé»˜è®¤
#         if not events:
#             events = [(0.0, 1.0)]
            
#         return events

#     def _build_graph_cot_prompt(self, question, options, segments, adj_matrix, selected_indices):
#         event_timeline = [f"Event{i+1}" for i, _, _ in segments]
        
#         # æ ¼å¼åŒ–é€‰é¡¹å­—ç¬¦ä¸²
#         if isinstance(options, list):
#             options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options)])
#         else:
#             options_str = str(options)

#         prompt = (
#             f"Question: {question}\n"
#             f"Options:\n{options_str}\n"
#             f"Key Events Timeline: {' -> '.join(event_timeline)}\n"
#             f"Based on these visual events, reason step-by-step and choose the best answer."
#         )
#         return prompt

#     def process_and_inference(self, video_path, question, options):
#         # 1. æ£€æµ‹äº‹ä»¶
#         events = self._detect_shot_boundaries(video_path)
#         if not events: return "C"
        
#         # 2. æå–ç‰¹å¾
#         global_feats, local_feats, frames = self._extract_event_features(video_path, events)
#         if len(frames) == 0: return "C"

#         # 3. å»ºå›¾
#         Pi = self._construct_event_graph(global_feats, local_feats, events)
        
#         # 4. é€‰å›¾
#         sel_idx = self._select_subgraph(Pi, question, global_feats, events)
#         if not sel_idx: sel_idx = [0] # Fallback
        
#         # 5. å‡†å¤‡æ¨ç†æ•°æ®
#         selected_frames = [frames[i] for i in sorted(sel_idx)]
#         selected_segments = [(events[i][0], events[i][1], i) for i in sorted(sel_idx)]
        
#         # 6. ç”Ÿæˆ Prompt
#         prompt = self._build_graph_cot_prompt(
#             question, options, 
#             selected_segments, 
#             Pi, sel_idx
#         )
        
#         # 7. è°ƒç”¨ VLM æ¨ç†
#         return self.model.generate(selected_frames, prompt, options)


# /root/icml2026/event_graph/code/eventgraph.py
import torch
import numpy as np
import cv2
import os
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from .base_method import BaseMethod
from .graph_builder import compute_similarity_matrix, compute_pagerank_matrix
from .celf_solver import CELFSelector

# === ä¿®æ”¹ 1: å¯¼å…¥ TransNetV2Detector ===
# å‡è®¾ä½ æŠŠä»£ç ä¿å­˜ä¸ºäº† transnet_detector.py
try:
    from .transnet_detector import TransNetV2Detector
except ImportError:
    print("âš ï¸ Warning: Could not import TransNetV2Detector. Make sure transnet_detector.py exists.")
    TransNetV2Detector = None

try:
    from decord import VideoReader, cpu
except ImportError:
    print("âš ï¸ Warning: decord not installed")
    VideoReader = None

class EventGraphLMM(BaseMethod):
    def __init__(self, args, model):
        super().__init__(args, model)
        
        # Params from Paper Section 4.1
        self.tau = 30.0  
        self.delta = 0.65 
        self.alpha = 0.15 
        self.lambda_param = 1.0 
        self.token_budget = args.token_budget
        
        # Detect token density for budget calculation
        backbone_name = getattr(args, 'backbone', '')
        if '34B' in backbone_name:
            self.tokens_per_frame = 576 
        elif 'Qwen' in backbone_name:
            # Qwenæ˜¯åŠ¨æ€åˆ†è¾¨ç‡ï¼Œè¿™é‡Œç»™ä¸€ä¸ªç»éªŒå¹³å‡å€¼
            self.tokens_per_frame = 512 
        else:
            self.tokens_per_frame = 256 # Video-LLaVA-7B default
            
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # 1. Load CLIP Once (ç”¨äºåç»­ç‰¹å¾æå–å’Œå»ºå›¾ï¼Œä¸ç”¨äºæ£€æµ‹äº†)
        self._load_clip_model()
        
        # === ä¿®æ”¹ 2: åˆå§‹åŒ– TransNet V2 ===
        if TransNetV2Detector is not None:
            print("ğŸš€ [EventGraph] Initializing TransNet V2 Detector...")
            self.shot_detector = TransNetV2Detector(device='cuda') # å¼ºåˆ¶ä½¿ç”¨cudaå¦‚æœå¯ç”¨
        else:
            self.shot_detector = None
            print("âŒ [EventGraph] TransNet V2 Detector not available. Will use fallback.")

    def _load_clip_model(self):
        # Uses local path if available to save download time
        local_path = "/root/hhq/models/clip-vit-large-patch14"
        model_name = local_path if os.path.exists(local_path) else "openai/clip-vit-large-patch14"
        
        try:
            self.clip_processor = CLIPProcessor.from_pretrained(model_name)
            self.clip_model = CLIPModel.from_pretrained(model_name).to(self.device)
            self.clip_model.eval()
        except Exception as e:
            print(f"Warning: Loading CLIP from {model_name} failed ({e}), trying openai default.")
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
            self.clip_model.eval()

    def _detect_shot_boundaries(self, video_path):
        """
        ä½¿ç”¨ TransNet V2 è¿›è¡Œé•œå¤´åˆ†å‰²
        """
        # å¦‚æœæ£€æµ‹å™¨æ²¡åˆå§‹åŒ–ï¼Œç›´æ¥å›é€€
        if self.shot_detector is None:
            return self._fallback_windows(video_path)

        try:
            # === ä¿®æ”¹ 3: ç›´æ¥è°ƒç”¨ detect_shots ===
            # TransNetV2Detector.detect_shots å·²ç»è¿”å›äº† [(start, end), ...] æ ¼å¼
            # ä¸éœ€è¦å†æ‰‹åŠ¨è½¬æ¢ boundaries_to_events
            events = self.shot_detector.detect_shots(video_path, threshold=0.5)
            
            # è¿‡æ»¤æçŸ­çš„å™ªå£°ç‰‡æ®µ (< 0.5s)
            events = [e for e in events if (e[1] - e[0]) >= 0.5]
            
            # å¦‚æœæ£€æµ‹åˆ°çš„äº‹ä»¶å¤ªå°‘ï¼Œè¯´æ˜å¯èƒ½æ˜¯é•¿é•œå¤´æˆ–è€…æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨å›é€€ç­–ç•¥
            if len(events) < 1: 
                print(f"  [EventGraph] Too few shots detected ({len(events)}), using fallback.")
                return self._fallback_windows(video_path)
            
            return events

        except Exception as e:
            print(f"  âŒ [EventGraph] TransNet V2 failed ({e}), using fallback.")
            return self._fallback_windows(video_path)

    # _boundaries_to_events å‡½æ•°ç°åœ¨å¯ä»¥åˆ é™¤äº†ï¼Œå› ä¸º TransNet ç±»å†…éƒ¨å¤„ç†äº†
    # ä½†ä¸ºäº†é˜²æ­¢æŸäº›å­ç±»ç»§æ‰¿è°ƒç”¨ï¼Œä½ å¯ä»¥ä¿ç•™å®ƒï¼Œæˆ–è€…ç›´æ¥åˆ é™¤ä»¥ä¿æŒä»£ç æ•´æ´ã€‚
    # è¿™é‡Œæˆ‘æŠŠå®ƒç§»é™¤äº†ã€‚

    def _extract_event_features(self, video_path, events):
        # Batch processing for CLIP
        if VideoReader is None: raise ImportError("decord required")
        vr = VideoReader(video_path, ctx=cpu(0))
        fps = vr.get_avg_fps()
        
        representative_frames = []
        valid_indices = []
        
        for idx, (start_t, end_t) in enumerate(events):
            mid_t = (start_t + end_t) / 2.0
            # å®‰å…¨æ£€æŸ¥é˜²æ­¢è¶Šç•Œ
            if mid_t * fps >= len(vr): 
                # å°è¯•å–æœ€åä¸€å¼ 
                frame_idx = len(vr) - 1
            else:
                frame_idx = min(len(vr) - 1, int(mid_t * fps))
            
            try:
                frame_np = vr[frame_idx].asnumpy()
                representative_frames.append(Image.fromarray(frame_np))
                valid_indices.append(idx)
            except Exception as e:
                print(f"Error extracting frame at {mid_t}s: {e}")
                continue
        
        if not representative_frames:
            return torch.tensor([]), torch.tensor([]), []

        # Batch Process
        batch_size = 32
        global_feats_list = []
        local_feats_list = []
        
        with torch.no_grad():
            for i in range(0, len(representative_frames), batch_size):
                batch = representative_frames[i : i+batch_size]
                inputs = self.clip_processor(images=batch, return_tensors="pt", padding=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # Global (CLS)
                g_feats = self.clip_model.get_image_features(**inputs)
                global_feats_list.append(g_feats)
                
                # Local (Patches)
                outputs = self.clip_model.vision_model(**inputs, output_hidden_states=True)
                l_feats = outputs.last_hidden_state[:, 1:, :] # Remove CLS
                local_feats_list.append(l_feats)
        
        if len(global_feats_list) == 0:
            return torch.tensor([]), torch.tensor([]), []

        global_feats = torch.cat(global_feats_list, dim=0)
        local_feats = torch.cat(local_feats_list, dim=0)
        
        return global_feats, local_feats, representative_frames

    def _construct_event_graph(self, global_feats, local_feats, events):
        """
        æ„å»ºè¯­ä¹‰-æ—¶åºå›¾
        """
        # ç¡®ä¿æ•°æ®åœ¨åŒä¸€è®¾å¤‡
        global_feats = global_feats.to(self.device)
        local_feats = local_feats.to(self.device)
        
        N = global_feats.shape[0]
        # 1. Compute Semantic Adjacency (Eq. 3, 4)
        adj_semantic = compute_similarity_matrix(
            global_feats, local_feats, 
            tau=self.tau, 
            event_times=events, 
            threshold=self.delta
        )
        # 2. Compute Reachability (PageRank, Eq. 6)
        Pi = compute_pagerank_matrix(adj_semantic, alpha=self.alpha)
        return Pi

    def _select_subgraph(self, Pi, question, global_feats, events):
        """
        CELF ç®—æ³•é€‰æ‹©å…³é”®å­å›¾
        """
        # 1. Encode Question
        inputs = self.clip_processor(text=[question], return_tensors="pt", padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            q_feat = self.clip_model.get_text_features(**inputs)
            q_feat = q_feat / q_feat.norm(dim=-1, keepdim=True)
        
        # 2. Calculate Query Relevance (Eq. 5)
        # Normalize global feats
        g_norm = global_feats / global_feats.norm(dim=-1, keepdim=True)
        relevance = torch.mm(g_norm, q_feat.t()).squeeze() # (N,)
        
        # Handle shape mismatch if only 1 event
        if relevance.dim() == 0:
            relevance = relevance.unsqueeze(0)
            
        relevance = torch.clamp(relevance, min=0.0) # ReLU
        
        # 3. Calculate Cost (Token consumption)
        # ç®€å•çš„çº¿æ€§ä»£ä»·: æ¯ä¸ªäº‹ä»¶æ¶ˆè€— tokens_per_frame
        costs = torch.full((len(events),), self.tokens_per_frame, device=self.device)
        
        # 4. CELF Selection
        selector = CELFSelector(Pi, relevance, costs, lambda_param=self.lambda_param)
        selected_indices = selector.select(budget=self.token_budget)
        
        return selected_indices

    def _fallback_windows(self, video_path):
        """
        å¦‚æœ TransNet å¤±è´¥ï¼Œä½¿ç”¨ç­‰é—´éš”åˆ‡ç‰‡
        """
        print(f"âš ï¸ [EventGraph] Using fallback windows for {os.path.basename(video_path)}")
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()
            duration = count / fps if fps > 0 else 0
        except:
            duration = 0
        
        if duration == 0:
            # å°è¯•ç”¨decordè·å–æ—¶é•¿
            try:
                vr = VideoReader(video_path, ctx=cpu(0))
                duration = len(vr) / vr.get_avg_fps()
            except:
                return [(0.0, 1.0)] # æœ€åçš„å…œåº•
        
        events = []
        # æ¯2ç§’åˆ‡ä¸€æ®µ (æ¯”ä¹‹å‰çš„é€»è¾‘ç¨å¾®å¯†é›†ä¸€ç‚¹ï¼Œä¿è¯è¦†ç›–)
        step = 2.0
        for t in np.arange(0, duration, step):
            events.append((t, min(t + step, duration)))
        
        # å¦‚æœè§†é¢‘æçŸ­
        if not events:
            events = [(0.0, min(1.0, duration))]
            
        return events

    def _build_graph_cot_prompt(self, question, options, segments, adj_matrix, selected_indices):
        event_timeline = [f"Event{i+1}" for i, _, _ in segments]
        
        # æ ¼å¼åŒ–é€‰é¡¹å­—ç¬¦ä¸²
        if isinstance(options, list):
            # å¤„ç†å¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
            options_clean = []
            for opt in options:
                if isinstance(opt, dict):
                    options_clean.append(str(opt))
                else:
                    options_clean.append(str(opt))
            
            # å¦‚æœæ˜¯A,B,C,Dæ ¼å¼
            if len(options_clean) > 0 and (options_clean[0].startswith('A') or len(options_clean) <= 5):
                 options_str = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options_clean)])
            else:
                 options_str = "\n".join(options_clean)
        else:
            options_str = str(options)

        prompt = (
            f"Question: {question}\n"
            f"Options:\n{options_str}\n"
            f"Key Events Timeline: {' -> '.join(event_timeline)}\n"
            f"Based on these visual events, reason step-by-step and choose the best answer."
        )
        return prompt

    def process_and_inference(self, video_path, question, options):
        # 1. æ£€æµ‹äº‹ä»¶ (TransNet V2)
        events = self._detect_shot_boundaries(video_path)
        if not events: return "C"
        
        # 2. æå–ç‰¹å¾
        global_feats, local_feats, frames = self._extract_event_features(video_path, events)
        if len(frames) == 0: return "C"

        # 3. å»ºå›¾
        Pi = self._construct_event_graph(global_feats, local_feats, events)
        
        # 4. é€‰å›¾
        sel_idx = self._select_subgraph(Pi, question, global_feats, events)
        if not sel_idx: sel_idx = [0] # Fallback
        
        # 5. å‡†å¤‡æ¨ç†æ•°æ®
        # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        valid_sel_idx = [i for i in sel_idx if i < len(frames)]
        if not valid_sel_idx: valid_sel_idx = [0]
        
        selected_frames = [frames[i] for i in sorted(valid_sel_idx)]
        selected_segments = [(events[i][0], events[i][1], i) for i in sorted(valid_sel_idx)]
        
        # 6. ç”Ÿæˆ Prompt
        prompt = self._build_graph_cot_prompt(
            question, options, 
            selected_segments, 
            Pi, valid_sel_idx
        )
        
        # 7. è°ƒç”¨ VLM æ¨ç†
        return self.model.generate(selected_frames, prompt, options)